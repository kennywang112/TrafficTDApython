{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8578c363-35ba-4e90-ba5a-882503b160ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from tdamapper.core import MapperAlgorithm\n",
    "from tdamapper.cover import CubicalCover\n",
    "from tdamapper.plot import MapperLayoutInteractive\n",
    "from tdamapper.clustering import FailSafeClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90d87251-d259-4f2b-94e5-5d408c40ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"./Data/NPA_TMA2_1.csv\", low_memory=False)[:-2]\n",
    "data2 = pd.read_csv(\"./Data/NPA_TMA2_2.csv\", low_memory=False)[:-2]\n",
    "data3 = pd.read_csv(\"./Data/NPA_TMA2_3.csv\", low_memory=False)[:-2]\n",
    "data4 = pd.read_csv(\"./Data/NPA_TMA2_4.csv\", low_memory=False)[:-2]\n",
    "dataA2 = pd.concat([data1, data2, data3, data4], ignore_index=True)\n",
    "\n",
    "dataA1 = pd.read_csv(\"./Data/NPA_TMA1.csv\")[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a69f8-0202-48da-8d35-933ed69b2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to select\n",
    "select_lst = [\n",
    "    # 天氣因素\n",
    "    # '天候名稱', '路面狀況-路面狀態名稱',\n",
    "    # # 人\n",
    "    # # '肇因研判子類別名稱-主要',\n",
    "    # '肇因研判大類別名稱-主要', '當事者屬-性-別名稱', '當事者事故發生時年齡', \n",
    "    # '當事者行動狀態大類別名稱', '車輛撞擊部位大類別名稱-最初', #'肇因研判大類別名稱-個別', #該欄位和主要一樣\n",
    "    # 其他\n",
    "    '光線名稱',\n",
    "    '道路類別-第1當事者-名稱', '速限-第1當事者', \n",
    "    '道路型態大類別名稱', '事故位置大類別名稱', \n",
    "    '路面狀況-路面鋪裝名稱',\n",
    "    '路面狀況-路面缺陷名稱', '道路障礙-障礙物名稱',\n",
    "    '道路障礙-視距品質名稱', '號誌-號誌種類名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱', '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '事故類型及型態大類別名稱',\n",
    "    '死亡受傷人數',\n",
    "    '經度', '緯度'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2f267-6edf-4b16-90e1-23863ad98a8e",
   "metadata": {},
   "source": [
    "# 景點處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86763d75-b482-44c4-a6d8-ea0664fa9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "scenic = pd.read_csv(\"./Data/Scenic_Spot_C_f.csv\", low_memory=False)\n",
    "\n",
    "dist_dfA1 = preprocess(dataA1, sample = 592)\n",
    "dist_dfA2 = preprocess(dataA2, sample = 5920)\n",
    "\n",
    "# 將經緯度轉換成球體的三維坐標\n",
    "def latlon_to_xyz(lat, lon):\n",
    "    lat_rad = np.radians(lat)\n",
    "    lon_rad = np.radians(lon)\n",
    "    x = np.cos(lat_rad) * np.cos(lon_rad)\n",
    "    y = np.cos(lat_rad) * np.sin(lon_rad)\n",
    "    z = np.sin(lat_rad)\n",
    "    return np.vstack((x, y, z)).T\n",
    "\n",
    "# 計算球面距離\n",
    "def spherical_dist(pos1, pos2, radius=6371):\n",
    "    cos_angle = np.dot(pos1, pos2.T)\n",
    "    angle = np.arccos(np.clip(cos_angle, -1, 1))\n",
    "    return radius * angle\n",
    "\n",
    "def calculate_distances(df, scenic_df, distance_threshold=1000):\n",
    "    # 將經緯度轉換成球體的三維坐標\n",
    "    df_xyz = latlon_to_xyz(df['緯度'], df['經度'])\n",
    "    scenic_xyz = latlon_to_xyz(scenic_df['Py'], scenic_df['Px'])\n",
    "\n",
    "    # 計算所有配對之間的球面距離，並轉換為米\n",
    "    distances = spherical_dist(df_xyz, scenic_xyz) * 1000\n",
    "\n",
    "    # 檢查哪些距離小於設定的threshold\n",
    "    distances_less_than_threshold = (distances < distance_threshold)\n",
    "\n",
    "    # 計算每一行小於閾值的點數量\n",
    "    df['景點數'] = distances_less_than_threshold.sum(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 使用函數，假設 dist_dfA1 和 scenic 已經被正確地載入\n",
    "scenic_dfA1 = calculate_distances(dist_dfA1[0], scenic)\n",
    "scenic_dfA2 = calculate_distances(dist_dfA2[0], scenic)\n",
    "\n",
    "rbind_data = pd.concat([scenic_dfA1, scenic_dfA2], axis=0, ignore_index=True)\n",
    "# 處理數值型資料\n",
    "rbind_data.loc[rbind_data['景點數'] > 1, '景點數'] = 2\n",
    "rbind_data.loc[rbind_data['受傷'] > 1, '受傷'] = 2\n",
    "rbind_data['速限-第1當事者'] = rbind_data['速限-第1當事者'].apply(lambda x: 1 if x > 60 else 0)\n",
    "\n",
    "dist_df = process_data(rbind_data)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "full_dist = pd.DataFrame(scaler.fit_transform(dist_df), columns = dist_df.columns)\n",
    "X1 = full_dist.drop(['受傷', '死亡', '經度', '緯度'], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8726599-0076-4b39-9a89-7b4348510c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens1 = PCA(10).fit_transform(X1)\n",
    "\n",
    "# mapper_algo1 = MapperAlgorithm(\n",
    "#     cover = CubicalCover(\n",
    "#         n_intervals = 5,\n",
    "#         overlap_frac = 0.5\n",
    "#     ),\n",
    "#     clustering = FailSafeClustering(\n",
    "#         clustering = AgglomerativeClustering(5, affinity='euclidean', linkage='ward'),\n",
    "#         verbose = False)\n",
    "# )\n",
    "\n",
    "# mapper_graph1 = mapper_algo1.fit_transform(X1, lens1)\n",
    "\n",
    "# mapper_plot1 = MapperLayoutInteractive(\n",
    "#     mapper_graph1,\n",
    "#     colors = dist_df[['景點數']].to_numpy(),\n",
    "#     cmap = 'jet',\n",
    "#     # agg = np.nanmean,\n",
    "#     agg = most_frequent_nonan,\n",
    "#     dim = 3,\n",
    "#     iterations = 30,\n",
    "#     seed = 5,\n",
    "#     width = 800,\n",
    "#     height = 500)\n",
    "\n",
    "# fig_mean1 = mapper_plot1.plot()\n",
    "# fig_mean1.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f53ce16e-cf57-43f7-9082-67c179628816",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vars(mapper_plot1._MapperLayoutInteractive__fig)['_data_objs'][1]['x']\n",
    "y = vars(mapper_plot1._MapperLayoutInteractive__fig)['_data_objs'][1]['y']\n",
    "z = vars(mapper_plot1._MapperLayoutInteractive__fig)['_data_objs'][1]['z']\n",
    "\n",
    "threeDimData = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "\n",
    "import re\n",
    "data_tuple = vars(mapper_plot1._MapperLayoutInteractive__fig)['_data_objs'][1]['text']\n",
    "\n",
    "data = []\n",
    "for item in data_tuple:\n",
    "    color = int(re.search(r'color: (\\d+)', item).group(1))\n",
    "    node = int(re.search(r'node: (\\d+)', item).group(1))\n",
    "    size = int(re.search(r'size: (\\d+)', item).group(1))\n",
    "    data.append({'color': color, 'node': node, 'size': size})\n",
    "component_info = pd.DataFrame(data)\n",
    "\n",
    "full_info = pd.concat([component_info, threeDimData], axis=1)\n",
    "\n",
    "mp_content_origin = vars(mapper_plot1._MapperLayoutInteractive__graph)['_node']\n",
    "\n",
    "mp_content = pd.DataFrame.from_dict(mp_content_origin, orient='index')\n",
    "mp_content.reset_index(inplace=True)\n",
    "mp_content.rename(columns={'index': 'node'}, inplace=True)\n",
    "\n",
    "full_info = pd.merge(full_info, mp_content, on=['node', 'size'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4fcb60-61c0-4c54-baf5-4ea6cd91c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# # calinski_data = get_calinski_from_db(full_info, 0.05)\n",
    "# calinski_data = get_calinski_from_db(full_info, 0.09)\n",
    "# labels = calinski_data[3]\n",
    "# db = calinski_data[2]\n",
    "# n_clusters_ = calinski_data[4]\n",
    "\n",
    "# unique_labels = set(labels)\n",
    "# core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "# core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# def matplotlib_to_plotly(cmap, alpha=1):\n",
    "#     \"\"\"rgba\"\"\"\n",
    "#     return f'rgba({int(cmap[0]*200)}, {int(cmap[1]*200)}, {int(cmap[2]*200)}, {alpha})'\n",
    "\n",
    "# # colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]  \n",
    "# colors = [matplotlib_to_plotly(plt.cm.Spectral(each), alpha=0.8) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "# fig = go.Figure()\n",
    "\n",
    "# for k, col in zip(unique_labels, colors):\n",
    "#     if k == -1:\n",
    "#         # col = 'rgba(0,0,0,0)'\n",
    "#         col = 'rgba(0,0,0,0)'\n",
    "\n",
    "#     class_member_mask = labels == k\n",
    "\n",
    "#     core_samples = full_info.iloc[:, 3:6][class_member_mask & core_samples_mask]\n",
    "#     fig.add_trace(go.Scatter3d(\n",
    "#         x=core_samples.iloc[:, 0],\n",
    "#         y=core_samples.iloc[:, 1],\n",
    "#         z=core_samples.iloc[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=6,\n",
    "#             color=col,\n",
    "#             opacity=0.8\n",
    "#         ),\n",
    "#         name=f'Cluster {k} Core'\n",
    "#     ))\n",
    "\n",
    "#     non_core_samples = full_info.iloc[:, 3:6][class_member_mask & ~core_samples_mask]\n",
    "#     fig.add_trace(go.Scatter3d(\n",
    "#         x=non_core_samples.iloc[:, 0],\n",
    "#         y=non_core_samples.iloc[:, 1],\n",
    "#         z=non_core_samples.iloc[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=6,\n",
    "#             color=col,\n",
    "#             opacity=0.5\n",
    "#         ),\n",
    "#         name=f'Cluster {k} Non-Core'\n",
    "#     ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=f\"Estimated number of clusters: {n_clusters_}\",\n",
    "#     margin=dict(l=0, r=0, b=0, t=0)\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ea28539-fe6c-414a-a6a6-eed14ac7aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1 -1]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(full_info['label'].unique())\n",
    "\n",
    "label_0 = full_info[full_info['label'] == 0]\n",
    "label_1 = full_info[full_info['label'] == 1]\n",
    "\n",
    "def get_count_dict(input_data):\n",
    "    count = {}\n",
    "    for key_to_search in input_data['node']:\n",
    "        ids_to_search = input_data[input_data['node'] == key_to_search]['ids'].values[0]\n",
    "        for num in ids_to_search:\n",
    "            if num in count:\n",
    "                count[num] += 1\n",
    "            else:\n",
    "                count[num] = 1\n",
    "                \n",
    "    return count\n",
    "\n",
    "count_0 = get_count_dict(label_0)\n",
    "count_1 = get_count_dict(label_1)\n",
    "\n",
    "both_cluster = {key: count_0[key] for key in count_0.keys() & count_1.keys()}\n",
    "\n",
    "full_0 = rbind_data.loc[count_0.keys()]\n",
    "full_1 = rbind_data.loc[count_1.keys()]\n",
    "\n",
    "lst = list(count_0.keys() & count_1.keys())\n",
    "full_12 = full_0.loc[lst]\n",
    "\n",
    "# 將重複的key另外拉出進行分析，這裡drop是為了符合卡方的獨立性前提假設\n",
    "full_0 = full_0.drop(lst)\n",
    "full_1 = full_1.drop(lst)\n",
    "\n",
    "# full_12_count = full\n",
    "print(len(lst))\n",
    "for key in lst:\n",
    "    del count_0[key]\n",
    "    del count_1[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d80e94-1871-4502-8c02-81e59342570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6281\n",
      "(6100, 21) (167, 21) (14, 21)\n",
      "30804 428 14\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def add_count(input_data, count):\n",
    "\n",
    "    input_data['count'] = 0\n",
    "\n",
    "    for key, value in count.items():\n",
    "        input_data.loc[key, 'count'] = value\n",
    "\n",
    "    return input_data\n",
    "\n",
    "full_0 = add_count(full_0, count_0)\n",
    "full_1 = add_count(full_1, count_1)\n",
    "full_12 = add_count(full_12, both_cluster)\n",
    "\n",
    "print(full_0.shape[0] + full_1.shape[0] + full_12.shape[0])\n",
    "print(full_0.shape, full_1.shape, full_12.shape)\n",
    "print(full_0['count'].sum(), full_1['count'].sum(), full_12['count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9480eee8-8b4a-4494-bc65-1ebae57ca290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster1</th>\n",
       "      <th>cluster2</th>\n",
       "      <th>cluster12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.449836</td>\n",
       "      <td>0.113772</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.371967</td>\n",
       "      <td>0.640719</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.178197</td>\n",
       "      <td>0.245509</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster1  cluster2  cluster12\n",
       "2  0.449836  0.113772   0.142857\n",
       "0  0.371967  0.640719   0.357143\n",
       "1  0.178197  0.245509   0.500000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def table(colnames):\n",
    "    \n",
    "    combined_df = pd.concat([full_0[colnames].value_counts(normalize = True), \n",
    "                             full_1[colnames].value_counts(normalize = True),\n",
    "                             full_12[colnames].value_counts(normalize = True)\n",
    "                            ],\n",
    "                            axis=1).fillna(0)\n",
    "\n",
    "    combined_df.columns = ['cluster1', 'cluster2', 'cluster12']\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "table('景點數')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2b15d2c-2efb-4e45-be9a-3e1eefade5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "光線名稱 p值: 0.011077062051727354 可分群\n",
      "道路類別-第1當事者-名稱 p值: 0.0 可分群\n",
      "速限-第1當事者 p值: 0.0 可分群\n",
      "道路型態大類別名稱 p值: 4.000681196272407e-20 可分群\n",
      "事故位置大類別名稱 p值: 3.451895007975873e-28 可分群\n",
      "路面狀況-路面鋪裝名稱 p值: 1.0 不可分群\n",
      "路面狀況-路面缺陷名稱 p值: 1.0 不可分群\n",
      "道路障礙-障礙物名稱 p值: 1.6350386914364776e-08 可分群\n",
      "道路障礙-視距品質名稱 p值: 1.0 不可分群\n",
      "號誌-號誌種類名稱 p值: 3.0958831600694374e-06 可分群\n",
      "車道劃分設施-分向設施大類別名稱 p值: 6.829118550875341e-91 可分群\n",
      "車道劃分設施-分道設施-快車道或一般車道間名稱 p值: 4.276895093863431e-106 可分群\n",
      "車道劃分設施-分道設施-快慢車道間名稱 p值: 0.000126247620814125 可分群\n",
      "車道劃分設施-分道設施-路面邊線名稱 p值: 4.904939835835238e-29 可分群\n",
      "事故類型及型態大類別名稱 p值: 8.09856997580718e-09 可分群\n",
      "經度 p值: 0.2802971882460983 不可分群\n",
      "緯度 p值: 0.3893669416369531 不可分群\n",
      "死亡 p值: 2.9800179510088593e-41 可分群\n",
      "受傷 p值: 1.2414727306044093e-14 可分群\n",
      "景點數 p值: 2.2134058366155242e-17 可分群\n"
     ]
    }
   ],
   "source": [
    "def chi_compare(c0, c1):\n",
    "    dict_0 = {}\n",
    "    dict_1 = {}\n",
    "    \n",
    "    for i in range(c1.shape[1] - 1):\n",
    "        dict_0[c0.columns[i]] = c0.iloc[:, i].value_counts()\n",
    "\n",
    "    for i in range(full_1.shape[1] - 1): \n",
    "        dict_1[c1.columns[i]] = c1.iloc[:, i].value_counts()\n",
    "        \n",
    "    pvalue_lst = [] \n",
    "    for i in range(full_1.shape[1] - 1):\n",
    "        combined_df = pd.concat([dict_0[c0.columns[i]], dict_1[c1.columns[i]]], axis=1).fillna(0)\n",
    "        combined_df.columns = ['cluster1', 'cluster2']\n",
    "        # print(combined_df)\n",
    "\n",
    "        # 將 DataFrame 轉換為列聯表\n",
    "        contingency_table = combined_df.values\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        if p > 0.05:\n",
    "            print(f\"{c1.columns[i]} p值: {p} 不可分群\")\n",
    "            # continue\n",
    "        else:\n",
    "            print(f\"{c1.columns[i]} p值: {p} 可分群\")\n",
    "            \n",
    "chi_compare(full_0, full_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
