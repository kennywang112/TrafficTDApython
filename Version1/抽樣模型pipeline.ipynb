{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fcbacd-0fa3-4e33-ae74-74513d5ab379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "from tdamapper.core import MapperAlgorithm\n",
    "from tdamapper.cover import CubicalCover\n",
    "from tdamapper.plot import MapperLayoutInteractive, MapperLayoutStatic\n",
    "from tdamapper.clustering import FailSafeClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "from functions import *\n",
    "from chi import *\n",
    "from regressionP import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db03b3f3-a74a-4a75-b4f5-0e3aa4e9eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"./Data/NPA_TMA2_1.csv\", low_memory=False)[:-2]\n",
    "data2 = pd.read_csv(\"./Data/NPA_TMA2_2.csv\", low_memory=False)[:-2]\n",
    "data3 = pd.read_csv(\"./Data/NPA_TMA2_3.csv\", low_memory=False)[:-2]\n",
    "data4 = pd.read_csv(\"./Data/NPA_TMA2_4.csv\", low_memory=False)[:-2]\n",
    "dataA2 = pd.concat([data1, data2, data3, data4], ignore_index=True)\n",
    "\n",
    "dataA1 = pd.read_csv(\"./Data/NPA_TMA1.csv\")[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a227e7-5562-4f3b-aefd-3db29ce8e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_data, select_lst, sample = 592):\n",
    "    sample_data = input_data[input_data['當事者順位'] == 1].reset_index(drop=True, inplace=False).sample(sample).reset_index(drop=True)\n",
    "    dataA = sample_data[select_lst]\n",
    "    \n",
    "    death_injury_data = split_death_injury(dataA['死亡受傷人數'])\n",
    "    dist_df = pd.concat([dataA, death_injury_data], axis=1)\n",
    "    dist_df.drop(columns=['死亡受傷人數'], inplace=True)\n",
    "    \n",
    "    return dist_df, sample_data\n",
    "\n",
    "# List of columns to select\n",
    "select_lst = [\n",
    "    '天候名稱', \n",
    "    '路面狀況-路面狀態名稱',\n",
    "    '肇因研判大類別名稱-主要', '當事者屬-性-別名稱', '當事者事故發生時年齡', \n",
    "    '車輛撞擊部位大類別名稱-最初',\n",
    "    '光線名稱',\n",
    "    '道路類別-第1當事者-名稱',\n",
    "    '速限-第1當事者', '道路障礙-視距品質名稱',\n",
    "    '道路型態大類別名稱', '事故位置大類別名稱',\n",
    "    '號誌-號誌種類名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱', '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '事故類型及型態大類別名稱',\n",
    "    '死亡受傷人數',\n",
    "    '經度', '緯度',\n",
    "    '道路型態子類別名稱', '事故位置子類別名稱', '車道劃分設施-分向設施子類別名稱', '事故類型及型態子類別名稱', \n",
    "    '當事者行動狀態子類別名稱', '車輛撞擊部位子類別名稱-最初', '車輛撞擊部位子類別名稱-其他', '肇因研判子類別名稱-個別',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c60eb-6340-41f8-9a45-3c7e2f968363",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "1. 抽樣\n",
    "2. 帶入mapper\n",
    "3. cluster\n",
    "4. 帶入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9831efb3-3613-4c6e-b73b-48e17be35cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_plot(A1, A2, selected, p_age = False):\n",
    "    dist_dfA1 = preprocess(dataA1, selected, sample = 592)\n",
    "    dist_dfA2 = preprocess(dataA2, selected, sample = 11940)\n",
    "\n",
    "    rbind_data = pd.concat([dist_dfA1[0], dist_dfA2[0]], axis=0, ignore_index=True)\n",
    "    \n",
    "    rbind_data.loc[rbind_data['受傷'] > 1, '受傷'] = 2\n",
    "    rbind_data['速限-第1當事者'] = rbind_data['速限-第1當事者'].apply(lambda x: 1 if x > 60 else 0)\n",
    "    if p_age == True:\n",
    "        rbind_data = process_age(rbind_data)\n",
    "\n",
    "    dist_df = process_data(rbind_data)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    full_dist = pd.DataFrame(scaler.fit_transform(dist_df), columns = dist_df.columns)\n",
    "    X1 = full_dist.drop(['受傷', '死亡', '經度', '緯度'], axis=1).to_numpy()\n",
    "\n",
    "    lens1 = PCA(10).fit_transform(X1)\n",
    "\n",
    "    mapper_algo1 = MapperAlgorithm(\n",
    "        cover = CubicalCover(\n",
    "            n_intervals = 3,\n",
    "            overlap_frac = 0.2\n",
    "        ),\n",
    "        clustering = FailSafeClustering(\n",
    "            clustering = AgglomerativeClustering(3, linkage='ward'),\n",
    "            verbose = False)\n",
    "    )\n",
    "    mapper_graph1 = mapper_algo1.fit_transform(X1, lens1)\n",
    "\n",
    "    mapper_plot1 = MapperLayoutInteractive(\n",
    "        mapper_graph1,\n",
    "        colors = rbind_data[['速限-第1當事者']].to_numpy(),\n",
    "        cmap = 'jet',\n",
    "        agg = np.nanmean,\n",
    "        # agg = most_frequent_nonan,\n",
    "        dim = 3,\n",
    "        iterations = 30,\n",
    "        seed = 15,\n",
    "        width = 800,\n",
    "        height = 500)\n",
    "    \n",
    "    return mapper_plot1, rbind_data\n",
    "\n",
    "def get_full_info(mapper_plot):\n",
    "    x = vars(mapper_plot._MapperLayoutInteractive__fig)['_data_objs'][1]['x']\n",
    "    y = vars(mapper_plot._MapperLayoutInteractive__fig)['_data_objs'][1]['y']\n",
    "    z = vars(mapper_plot._MapperLayoutInteractive__fig)['_data_objs'][1]['z']\n",
    "\n",
    "    threeDimData = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "\n",
    "    import re\n",
    "    data_tuple = vars(mapper_plot._MapperLayoutInteractive__fig)['_data_objs'][1]['text']\n",
    "\n",
    "    data = []\n",
    "    for item in data_tuple:\n",
    "        color = int(re.search(r'color: (\\d+)', item).group(1))\n",
    "        node = int(re.search(r'node: (\\d+)', item).group(1))\n",
    "        size = int(re.search(r'size: (\\d+)', item).group(1))\n",
    "        data.append({'color': color, 'node': node, 'size': size})\n",
    "    component_info = pd.DataFrame(data)\n",
    "\n",
    "    full_info = pd.concat([component_info, threeDimData], axis=1)\n",
    "\n",
    "    mp_content_origin = vars(mapper_plot._MapperLayoutInteractive__graph)['_node']\n",
    "\n",
    "    mp_content = pd.DataFrame.from_dict(mp_content_origin, orient='index')\n",
    "    mp_content.reset_index(inplace=True)\n",
    "    mp_content.rename(columns={'index': 'node'}, inplace=True)\n",
    "\n",
    "    full_info = pd.merge(full_info, mp_content, on=['node', 'size'], how='inner')\n",
    "    full_info.iloc[:, 3:6]\n",
    "    \n",
    "    return full_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "647c1185-5e49-4b88-b771-6a66c2d59952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper_plot1, rbind_data, full_info, calinski_data = process_with_adjusted_threshold(dataA1, dataA2, select_lst)\n",
    "# mapper_plot1, rbind_data = resample_plot(dataA1, dataA2, select_lst)\n",
    "# fig_mean1 = mapper_plot1.plot()\n",
    "# fig_mean1.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdebf2fb-763a-40ee-bb8b-4aa0cf1d905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_info = get_full_info(mapper_plot1)\n",
    "# calinski_data = get_calinski_from_db(full_info, 0.01)\n",
    "# print(calinski_data[0])\n",
    "\n",
    "# labels = calinski_data[3]\n",
    "# db = calinski_data[2]\n",
    "# n_clusters_ = calinski_data[4]\n",
    "\n",
    "# unique_labels = set(labels)\n",
    "# core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "# core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# def matplotlib_to_plotly(cmap, alpha=1):\n",
    "#     \"\"\"rgba\"\"\"\n",
    "#     return f'rgba({int(cmap[0]*255)}, {int(cmap[1]*255)}, {int(cmap[2]*255)}, {alpha})'\n",
    "\n",
    "# # colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]  \n",
    "# colors = [matplotlib_to_plotly(plt.cm.Spectral(each), alpha=0.8) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "# fig = go.Figure()\n",
    "\n",
    "# for k, col in zip(unique_labels, colors):\n",
    "#     if k == -1:\n",
    "#         # col = 'rgba(0,0,0,0)'\n",
    "#         col = 'rgba(0,0,0,0)'\n",
    "\n",
    "#     class_member_mask = labels == k\n",
    "\n",
    "#     core_samples = full_info.iloc[:, 3:6][class_member_mask & core_samples_mask]\n",
    "#     fig.add_trace(go.Scatter3d(\n",
    "#         x=core_samples.iloc[:, 0],\n",
    "#         y=core_samples.iloc[:, 1],\n",
    "#         z=core_samples.iloc[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=6,\n",
    "#             color=col,\n",
    "#             opacity=0.8\n",
    "#         ),\n",
    "#         name=f'Cluster {k} Core'\n",
    "#     ))\n",
    "\n",
    "#     non_core_samples = full_info.iloc[:, 3:6][class_member_mask & ~core_samples_mask]\n",
    "#     fig.add_trace(go.Scatter3d(\n",
    "#         x=non_core_samples.iloc[:, 0],\n",
    "#         y=non_core_samples.iloc[:, 1],\n",
    "#         z=non_core_samples.iloc[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=6,\n",
    "#             color=col,\n",
    "#             opacity=0.5\n",
    "#         ),\n",
    "#         name=f'Cluster {k} Non-Core'\n",
    "#     ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=f\"Estimated number of clusters: {n_clusters_}\",\n",
    "#     margin=dict(l=0, r=0, b=0, t=0)\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f58c4acc-e0ae-4c54-a300-638793b48699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_counts(full_info):\n",
    "    label_0 = full_info[full_info['label'] == 0]\n",
    "    label_1 = full_info[full_info['label'] == 1]\n",
    "    label_out = full_info[(full_info['label'] != 1) & (full_info['label'] != 0)]\n",
    "\n",
    "    count_0 = get_count_dict(label_0)\n",
    "    count_1 = get_count_dict(label_1)\n",
    "    count_out = get_count_dict(label_out)\n",
    "\n",
    "#     print(full_info['label'].unique())\n",
    "\n",
    "#     print(len(count_0), len(count_1), len(count_2))\n",
    "    \n",
    "    full_0 = rbind_data.loc[count_0.keys()]\n",
    "    full_1 = rbind_data.loc[count_1.keys()]\n",
    "    # 離群值不需要被處理\n",
    "    full_out = rbind_data.loc[count_out.keys()]\n",
    "\n",
    "    lst01 = list(count_0.keys() & count_1.keys())\n",
    "    lsto0 = list(count_out.keys() & count_0.keys())\n",
    "    lsto1 = list(count_out.keys() & count_1.keys())\n",
    "\n",
    "    # 將重複的key另外拉出進行分析，這裡drop是為了符合卡方的獨立性前提假設\n",
    "    full_01 = full_0.loc[lst01]\n",
    "\n",
    "    full_combine = pd.concat([full_01, full_out], axis=0)\n",
    "\n",
    "    # 四掉連接點，使分析更嚴謹\n",
    "    full_0 = full_0.drop(lst01, errors='ignore')\n",
    "    full_0 = full_0.drop(lsto0, errors='ignore')\n",
    "\n",
    "    full_1 = full_1.drop(lst01, errors='ignore')\n",
    "    full_1 = full_1.drop(lsto1, errors='ignore')\n",
    "\n",
    "    # print(full_0.shape, full_1.shape, full_2.shape)\n",
    "    # print('01連接點數量', len(lst01))\n",
    "    # print('02連接點數量', len(lst02))\n",
    "    # print('12連接點數量', len(lst12))\n",
    "    # print('o0連接點數量', len(lsto0))\n",
    "    # print('o1連接點數量', len(lsto1))\n",
    "    # print('o2連接點數量', len(lsto2))\n",
    "    # print('離群值數量', full_out.shape[0])\n",
    "    \n",
    "    return full_0, full_1, full_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc315b0b-e245-4368-b819-207b3dff5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusterN_logit(cluster_data, lst):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    c0_for_lm = process_data(cluster_data)\n",
    "    c0_for_lm_X = pd.DataFrame(scaler.fit_transform(c0_for_lm), columns=c0_for_lm.columns)\n",
    "    \n",
    "    # 設置三個等級的label\n",
    "    # c0_for_lm_y = cluster_data['受傷']\n",
    "    # c0_for_lm_y = cluster_data.apply(lambda row: 2 if row['受傷'] >= 2 else 1, axis=1)\n",
    "    c0_for_lm_y = cluster_data.apply(lambda row: 2 if row['死亡'] != 0 else (2 if row['受傷'] >= 2 else 1), axis=1)\n",
    "        \n",
    "    c0_for_lm_X = c0_for_lm_X[lst]\n",
    "    \n",
    "    return c0_for_lm_X, c0_for_lm_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f5904-79fe-452a-85a7-4389d72490f3",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5539b38f-fdd4-4cef-b3ce-a50a8f49d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_with_adjusted_threshold(dataA1, dataA2, select_lst, initial_threshold=0.03, min_threshold=0.1, decrement=0.01):\n",
    "#     threshold = initial_threshold\n",
    "#     while threshold >= min_threshold:\n",
    "#         mapper_plot1, rbind_data = resample_plot(dataA1, dataA2, select_lst)\n",
    "#         full_info = get_full_info(mapper_plot1)\n",
    "#         calinski_data = get_calinski_from_db(full_info, threshold)\n",
    "\n",
    "#         if len(full_info['label'].unique()) >= 3:\n",
    "#             break  # 停止迴圈，因為已達到條件\n",
    "#         threshold -= decrement\n",
    "\n",
    "#     mapper_plot1, rbind_data = resample_plot(dataA1, dataA2, select_lst)\n",
    "#     full_info = get_full_info(mapper_plot1)\n",
    "#     calinski_data = get_calinski_from_db(full_info, threshold)\n",
    "\n",
    "#     return mapper_plot1, rbind_data, full_info, calinski_data\n",
    "\n",
    "def process_with_adjusted_threshold(dataA1, dataA2, select_lst, initial_threshold=0.03, min_threshold=0.15, decrement=0.01):\n",
    "\n",
    "    mapper_plot1, rbind_data = resample_plot(dataA1, dataA2, select_lst)\n",
    "    full_info = get_full_info(mapper_plot1)\n",
    "    calinski_data = get_calinski_from_db(full_info, initial_threshold)\n",
    "\n",
    "    return mapper_plot1, rbind_data, full_info, calinski_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "583679ae-aba3-4f0a-b007-fb2de40ebea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_logit = [\n",
    "    '天候名稱', \n",
    "    '路面狀況-路面狀態名稱',\n",
    "    '肇因研判大類別名稱-主要', '當事者屬-性-別名稱', '當事者事故發生時年齡', \n",
    "    '車輛撞擊部位大類別名稱-最初',\n",
    "    '光線名稱',\n",
    "    '道路類別-第1當事者-名稱',\n",
    "    '速限-第1當事者',\n",
    "    '道路型態大類別名稱', '事故位置大類別名稱',\n",
    "    '號誌-號誌種類名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱', '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '事故類型及型態大類別名稱',\n",
    "    # '死亡受傷人數',\n",
    "    # '經度', '緯度',\n",
    "    '道路型態子類別名稱', '事故位置子類別名稱', '車道劃分設施-分向設施子類別名稱', '事故類型及型態子類別名稱', \n",
    "    '當事者行動狀態子類別名稱', '車輛撞擊部位子類別名稱-最初', '車輛撞擊部位子類別名稱-其他', '肇因研判子類別名稱-個別',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "855c5fc9-7df2-4d02-afc2-5284f64de61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TDA_scores_lg = []\n",
    "TDA_scores_rf = []\n",
    "TDA_scores_svm = []\n",
    "Origin_scores_lg = []\n",
    "Origin_scores_rf = []\n",
    "Origin_scores_svm = []\n",
    "\n",
    "# for i in range(2):\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     mapper_plot1, rbind_data, full_info, calinski_data = process_with_adjusted_threshold(dataA1, dataA2, select_lst)\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(elapsed_time)\n",
    "\n",
    "#     full_0, full_1, full_combine = get_counts(full_info)\n",
    "\n",
    "#     # print(full_combine.shape[0] + full_0.shape[0] + full_1.shape[0] + full_2.shape[0] == rbind_data.shape[0])\n",
    "\n",
    "#     full_combine_X,  full_combine_y = get_clusterN_logit(full_combine, lst_logit)\n",
    "#     full_0_X, full_0_y = get_clusterN_logit(full_0, lst_logit)\n",
    "#     full_1_X, full_1_y = get_clusterN_logit(full_1, lst_logit)\n",
    "\n",
    "#     # 帶入模型\n",
    "#     matrix_0, score_0 = logistic_cm_gridsearch(full_0_X, full_0_y)\n",
    "#     rf_matrix_0, rf_score_0 = decision_tree_cm_with_gridsearch(full_0_X, full_0_y)\n",
    "#     svc_matrix_0, svc_score_0 = svc_cm_with_grid_search(full_0_X, full_0_y)\n",
    "\n",
    "#     matrix_1, score_1 = logistic_cm_gridsearch(full_1_X, full_1_y)\n",
    "#     rf_matrix_1, rf_score_1 = decision_tree_cm_with_gridsearch(full_1_X, full_1_y)\n",
    "#     svc_matrix_1, svc_score_1 = svc_cm_with_grid_search(full_1_X, full_1_y)\n",
    "\n",
    "#     matrix_combine, score_combine = logistic_cm_gridsearch(full_combine_X, full_combine_y)\n",
    "#     rf_matrix_combine, rf_score_combine = decision_tree_cm_with_gridsearch(full_combine_X, full_combine_y)\n",
    "#     svc_matrix_combine, svc_score_combine = svc_cm_with_grid_search(full_combine_X, full_combine_y)\n",
    "\n",
    "#     de = full_0_X.shape[0] + full_1_X.shape[0] + full_2_X.shape[0]\n",
    "#     logit_avg_score = (full_0_X.shape[0]/de)*score_0 + (full_1_X.shape[0]/de)*score_1 + (full_combine_X.shape[0]/de)*score_combine\n",
    "#     rf_avg_score = (full_0_X.shape[0]/de)*rf_score_0 + (full_1_X.shape[0]/de)*rf_score_1 + (full_combine_X.shape[0]/de)*rf_score_combine\n",
    "#     svc_avg_score = (full_0_X.shape[0]/de)*svc_score_0 + (full_1_X.shape[0]/de)*svc_score_1 + (full_combine_X.shape[0]/de)*svc_score_combine\n",
    "#     print(f'Logistic : {round(logit_avg_score, 3)}\\nRF : {round(rf_avg_score, 3)}\\nSVM : {round(svc_avg_score, 3)}')\n",
    "\n",
    "#     origin_X, origin_y = get_clusterN_logit(rbind_data, lst_logit)\n",
    "#     matrix_origin, score_origin = logistic_cm(origin_X, origin_y)\n",
    "#     rf_matrix_origin, rf_score_origin = decision_tree_cm(origin_X, origin_y)\n",
    "#     svc_matrix_origin, svc_score_origin = svc_cm(origin_X, origin_y)\n",
    "#     print(f'Logistic : {round(score_origin, 3)}\\nRF : {round(rf_score_origin, 3)}\\nSVM : {round(svc_score_origin, 3)}')\n",
    "    \n",
    "#     TDA_scores_lg.append(round(logit_avg_score, 3))\n",
    "#     TDA_scores_rf.append(round(rf_avg_score, 3))\n",
    "#     TDA_scores_svm.append(round(svc_avg_score, 3))\n",
    "#     Origin_scores_lg.append(round(score_origin, 3))\n",
    "#     Origin_scores_rf.append(round(rf_score_origin, 3))\n",
    "#     Origin_scores_svm.append(round(svc_score_origin, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e462759-755a-404b-b149-96246d4496de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svc_cm_with_grid_search(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Set up the parameter grid\n",
    "    param_grid = {\n",
    "        'C': [1],\n",
    "        'multi_class': ['ovr', 'crammer_singer']\n",
    "    }\n",
    "\n",
    "    # Initialize the GridSearchCV object\n",
    "    grid_search = GridSearchCV(LinearSVC(penalty='l2', dual=True, fit_intercept=True, random_state=42, max_iter=3000), param_grid, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate confusion matrix and accuracy\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Prepare detailed performance metrics\n",
    "    cm_df = pd.DataFrame(conf_matrix, index=[f'Actual_{i}' for i in range(conf_matrix.shape[0])], columns=[f'Predicted_{i}' for i in range(conf_matrix.shape[1])])\n",
    "    precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "    metrics = {\n",
    "        'Label': [f'Class_{i}' for i in range(len(precision))],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    return metrics_df, accuracy\n",
    "\n",
    "def logistic_cm_gridsearch(X, y):\n",
    "    # 分割資料集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 使用RandomOverSampler來平衡資料\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 建立邏輯回歸模型並使用GridSearchCV來找到最佳參數\n",
    "    model = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=1000)\n",
    "    parameters = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.1, 1, 10]\n",
    "    }\n",
    "    grid_search = GridSearchCV(model, parameters, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # 使用最佳模型進行預測\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # 生成並打印混淆矩陣和各項度量指標\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Label': [f'Class_{i}' for i in range(len(precision))],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "    return metrics_df, accuracy\n",
    "\n",
    "def decision_tree_cm_with_gridsearch(X, y):\n",
    "    # Splitting the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Handling class imbalance\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Defining the model and parameters for grid search\n",
    "    dt_model = RandomForestClassifier(random_state=43)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'min_samples_leaf': [2, 4]\n",
    "    }\n",
    "\n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_resampled_scaled, y_resampled)\n",
    "\n",
    "    # Predicting labels for the test set\n",
    "    y_pred = grid_search.predict(X_test_scaled)\n",
    "\n",
    "    # Calculating the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Formatting the confusion matrix for display\n",
    "    cm_df = pd.DataFrame(conf_matrix, index=[f'Actual_{i}' for i in range(conf_matrix.shape[0])], \n",
    "                         columns=[f'Predicted_{i}' for i in range(conf_matrix.shape[1])])\n",
    "    precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "    metrics = {\n",
    "        'Label': [f'Class_{i}' for i in range(len(precision))],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    return metrics_df, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2689e77-df97-4c2e-880b-3e747483b413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1612   23]\n",
      " [ 677    8]]\n",
      "[[1526  109]\n",
      " [ 613   72]]\n",
      "[[1635    0]\n",
      " [ 685    0]]\n",
      "[[33  1]\n",
      " [14  1]]\n",
      "[[28  6]\n",
      " [13  2]]\n",
      "[[33  1]\n",
      " [14  1]]\n",
      "[[17  1]\n",
      " [13  1]]\n",
      "[[17  1]\n",
      " [10  4]]\n",
      "[[18  0]\n",
      " [14  0]]\n",
      "Logistic : 0.697\n",
      "RF : 0.688\n",
      "SVM : 0.703\n",
      "[[1671   29]\n",
      " [ 688   12]]\n",
      "[[1529  171]\n",
      " [ 619   81]]\n",
      "[[1700    0]\n",
      " [ 700    0]]\n",
      "Logistic : 0.701\n",
      "RF : 0.671\n",
      "SVM : 0.708\n"
     ]
    }
   ],
   "source": [
    "matrix_0, score_0,  cm_0 = logistic_cm_gridsearch(full_0_X, full_0_y)\n",
    "rf_matrix_0, rf_score_0, rf_cm_0 = rf_with_gridsearch(full_0_X, full_0_y)\n",
    "svc_matrix_0, svc_score_0, svc_cm_0 = svc_cm_with_grid_search(full_0_X, full_0_y)\n",
    "\n",
    "matrix_1, score_1, cm_1 = logistic_cm_gridsearch(full_1_X, full_1_y)\n",
    "rf_matrix_1, rf_score_1, rf_cm_1 = rf_with_gridsearch(full_1_X, full_1_y)\n",
    "svc_matrix_1, svc_score_1, svc_cm_1 = svc_cm_with_grid_search(full_1_X, full_1_y)\n",
    "\n",
    "matrix_2, score_2, cm_2 = logistic_cm_gridsearch(full_2_X, full_2_y)\n",
    "rf_matrix_2, rf_score_2, rf_cm_2 = rf_with_gridsearch(full_2_X, full_2_y)\n",
    "svc_matrix_2, svc_score_2, svc_cm_2 = svc_cm_with_grid_search(full_2_X, full_2_y)\n",
    "\n",
    "de = full_0_X.shape[0] + full_1_X.shape[0] + full_2_X.shape[0]\n",
    "logit_avg_score = (full_0_X.shape[0]/de)*score_0 + (full_1_X.shape[0]/de)*score_1 + (full_2_X.shape[0]/de)*score_cb\n",
    "rf_avg_score = (full_0_X.shape[0]/de)*rf_score_0 + (full_1_X.shape[0]/de)*rf_score_1 + (full_2_X.shape[0]/de)*rf_score_cb\n",
    "svc_avg_score = (full_0_X.shape[0]/de)*svc_score_0 + (full_1_X.shape[0]/de)*svc_score_1 + (full_2_X.shape[0]/de)*svm_score_cb\n",
    "print(f'Logistic : {round(logit_avg_score, 3)}\\nRF : {round(rf_avg_score, 3)}\\nSVM : {round(svc_avg_score, 3)}')\n",
    "\n",
    "origin_X, origin_y = get_clusterN_logit(rbind_data, lst_logit)\n",
    "\n",
    "matrix_origin, score_origin, cm_origin = logistic_cm_gridsearch(origin_X, origin_y)\n",
    "rf_matrix_origin, rf_score_origin, rf_cm_origin = rf_with_gridsearch(origin_X, origin_y)\n",
    "svc_matrix_origin, svc_score_origin, svc_cm_origin = svc_cm_with_grid_search(origin_X, origin_y)\n",
    "print(f'Logistic : {round(score_origin, 3)}\\nRF : {round(rf_score_origin, 3)}\\nSVM : {round(svc_score_origin, 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
