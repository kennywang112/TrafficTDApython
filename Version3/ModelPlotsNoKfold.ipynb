{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\System\\\\Library\\\\Fonts\\\\PingFang.ttc'\n"
     ]
    }
   ],
   "source": [
    "from utils.model_score import *\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    myfont = FontProperties(fname=r\"/System/Library/Fonts/PingFang.ttc\")\n",
    "    sns.set(style=\"whitegrid\", font=myfont.get_name())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 都是加入子類別的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ModelPerformance/pass_outlier_performance_log.pkl\", \"rb\") as f:\n",
    "    pass_outlier_performance_log = pickle.load(f)\n",
    "with open(\"./ModelPerformance/pass_performance_log.pkl\", \"rb\") as f:\n",
    "    pass_performance_log = pickle.load(f)\n",
    "with open(\"./ModelPerformance/car_motor_outlier_overlap_performance_log.pkl\", \"rb\") as f:\n",
    "    car_motor_outlier_overlap_performance_log = pickle.load(f)\n",
    "with open(\"./ModelPerformance/motor_performance_log.pkl\", \"rb\") as f:\n",
    "    motor_performance_log = pickle.load(f)\n",
    "with open(\"./ModelPerformance/car_performance_log.pkl\", \"rb\") as f:\n",
    "    car_performance_log = pickle.load(f)\n",
    "with open(\"./ModelPerformance/full_performance_log.pkl\", \"rb\") as f:\n",
    "    full_performance_log = pickle.load(f)\n",
    "    \n",
    "# 行人資料\n",
    "poo_mtrx, poo_recall, poo_class, poo_threshold = get_score(\n",
    "    pass_outlier_performance_log['y'], pass_outlier_performance_log['decision_scores'], threshold=0)\n",
    "p_mtrx, p_recall, p_class, p_threshold = get_score(\n",
    "    pass_performance_log['y'], pass_performance_log['decision_scores'])\n",
    "# 駕駛資料\n",
    "cmoo_mtrx, cmoo_recall, cmoo_class, cmoo_threshold = get_score(\n",
    "    car_motor_outlier_overlap_performance_log['y'], car_motor_outlier_overlap_performance_log['decision_scores'])\n",
    "m_mtrx, m_recall, m_class, m_threshold = get_score(\n",
    "    motor_performance_log['y'], motor_performance_log['decision_scores'])\n",
    "c_mtrx, c_recall, c_class, c_threshold = get_score(\n",
    "    car_performance_log['y'], car_performance_log['decision_scores'])\n",
    "# 整筆資料\n",
    "f_mtrx, f_recall, f_class, f_threshold = get_score(\n",
    "    full_performance_log['y'], full_performance_log['decision_scores'])\n",
    "\n",
    "# Load data for the second version\n",
    "with open(\"./ModelPerformanceV2/pass_outlier_performance_log.pkl\", \"rb\") as f:\n",
    "    pass_outlier_performance_log_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/pass_performance_log.pkl\", \"rb\") as f:\n",
    "    pass_performance_log_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/car_motor_outlier_overlap_performance_log.pkl\", \"rb\") as f:\n",
    "    car_motor_outlier_overlap_performance_log_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/motor_performance_log.pkl\", \"rb\") as f:\n",
    "    motor_performance_log_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/car_performance_log.pkl\", \"rb\") as f:\n",
    "    car_performance_log_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/full_performance_log.pkl\", \"rb\") as f:\n",
    "    full_performance_log_v2 = pickle.load(f)\n",
    "\n",
    "# Calculate recall scores for the second version\n",
    "poo_mtrx_v2, poo_recall_v2, poo_class_v2, poo_threshold_v2 = get_score(\n",
    "    pass_outlier_performance_log_v2['y'], pass_outlier_performance_log_v2['decision_scores'], threshold=0)\n",
    "p_mtrx_v2, p_recall_v2, p_class_v2, p_threshold_v2 = get_score(\n",
    "    pass_performance_log_v2['y'], pass_performance_log_v2['decision_scores'])\n",
    "cmoo_mtrx_v2, cmoo_recall_v2, cmoo_class_v2, cmoo_threshold_v2 = get_score(\n",
    "    car_motor_outlier_overlap_performance_log_v2['y'], car_motor_outlier_overlap_performance_log_v2['decision_scores'])\n",
    "m_mtrx_v2, m_recall_v2, m_class_v2, m_threshold_v2 = get_score(\n",
    "    motor_performance_log_v2['y'], motor_performance_log_v2['decision_scores'])\n",
    "c_mtrx_v2, c_recall_v2, c_class_v2, c_threshold_v2 = get_score(\n",
    "    car_performance_log_v2['y'], car_performance_log_v2['decision_scores'])\n",
    "f_mtrx_v2, f_recall_v2, f_class_v2, f_threshold_v2 = get_score(\n",
    "    full_performance_log_v2['y'], full_performance_log_v2['decision_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full version\n",
      "Top recall: 0.6850\n",
      "Bottom recall: 0.7874\n",
      "topology version\n",
      "Top recall: 0.6000\n",
      "Bottom recall: 0.7929\n"
     ]
    }
   ],
   "source": [
    "recall_top_log_f = f_mtrx_v2[0, 0] / (f_mtrx_v2[0, 0] + f_mtrx_v2[0, 1])\n",
    "recall_bottom_log_f = f_mtrx_v2[1, 1] / (f_mtrx_v2[1, 1] + f_mtrx_v2[1, 0])\n",
    "\n",
    "print('full version')\n",
    "print(f\"Top recall: {recall_top_log_f:.4f}\")\n",
    "print(f\"Bottom recall: {recall_bottom_log_f:.4f}\")\n",
    "\n",
    "mapper_log = c_mtrx_v2 + m_mtrx + cmoo_mtrx_v2 + p_mtrx_v2 + poo_mtrx_v2\n",
    "\n",
    "recall_top_log_m = mapper_log[0, 0] / (mapper_log[0, 0] + mapper_log[0, 1])\n",
    "recall_bottom_log_m = mapper_log[1, 1] / (mapper_log[1, 1] + mapper_log[1, 0])\n",
    "\n",
    "print('topology version')\n",
    "print(f\"Top recall: {recall_top_log_m:.4f}\")\n",
    "print(f\"Bottom recall: {recall_bottom_log_m:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取總數\n",
    "poo_total_v2 = np.sum(poo_mtrx_v2)\n",
    "p_total_v2 = np.sum(p_mtrx_v2)\n",
    "cmoo_total_v2 = np.sum(cmoo_mtrx_v2)\n",
    "m_total_v2 = np.sum(m_mtrx_v2)\n",
    "c_total_v2 = np.sum(c_mtrx_v2)\n",
    "f_total_v2 = np.sum(f_mtrx_v2)\n",
    "\n",
    "# Example recall values and names for the first version\n",
    "recalls_v1 = [poo_recall, p_recall, cmoo_recall, m_recall, c_recall, f_recall]\n",
    "names_v1 = ['po', 'pass', 'cmoo', 'motor', 'car', 'full']\n",
    "\n",
    "# Recall values for the second version\n",
    "recalls_v2 = [poo_recall_v2, p_recall_v2, cmoo_recall_v2, m_recall_v2, c_recall_v2, f_recall_v2]\n",
    "\n",
    "# Total values for the second version\n",
    "totals_v2 = [poo_total_v2, p_total_v2, cmoo_total_v2, m_total_v2, c_total_v2, f_total_v2]\n",
    "\n",
    "# Create the combined bar chart\n",
    "x = np.arange(len(names_v1))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars1 = ax.barh(x - width/2, recalls_v1, width, label='大類別', color='skyblue')\n",
    "bars2 = ax.barh(x + width/2, recalls_v2, width, label='加上小類別', color='lightcoral')\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        xval = bar.get_width()\n",
    "        plt.text(xval, bar.get_y() + bar.get_height()/2.0, f'{xval:.4f}', ha='left', va='center')\n",
    "\n",
    "# Add total values on the right side of the bars\n",
    "for i, total in enumerate(totals_v2):\n",
    "    plt.text(1.02, i + width/2, f'{total}', ha='left', va='center', fontsize=12, color='black')\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Recall Scores Comparison', fontsize=16)\n",
    "ax.set_xlabel('Recall', fontsize=14)\n",
    "ax.set_ylabel('Models', fontsize=14)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(names_v1)\n",
    "ax.set_xlim(0, 1)  # Ensure the x-axis starts at 0 and ends at 1\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ModelPerformance/pass_outlier_performance_svc.pkl\", \"rb\") as f:\n",
    "    pass_outlier_performance_svc = pickle.load(f)\n",
    "with open(\"./ModelPerformance/pass_performance_log.pkl\", \"rb\") as f:\n",
    "    pass_performance_svc = pickle.load(f)\n",
    "with open(\"./ModelPerformance/car_motor_outlier_overlap_performance_svc.pkl\", \"rb\") as f:\n",
    "    car_motor_outlier_overlap_performance_svc = pickle.load(f)\n",
    "with open(\"./ModelPerformance/motor_performance_svc.pkl\", \"rb\") as f:\n",
    "    motor_performance_svc = pickle.load(f)\n",
    "with open(\"./ModelPerformance/car_performance_svc.pkl\", \"rb\") as f:\n",
    "    car_performance_svc = pickle.load(f)\n",
    "with open(\"./ModelPerformance/full_performance_svc.pkl\", \"rb\") as f:\n",
    "    full_performance_svc = pickle.load(f)\n",
    "    \n",
    "# 行人資料\n",
    "poo_mtrx, poo_recall, poo_class, poo_threshold = get_score(\n",
    "    pass_outlier_performance_svc['y'], pass_outlier_performance_svc['decision_scores'], threshold=-5)\n",
    "p_mtrx, p_recall, p_class, p_threshold = get_score(\n",
    "    pass_performance_svc['y'], pass_performance_svc['decision_scores'])\n",
    "# 駕駛資料\n",
    "cmoo_mtrx, cmoo_recall, cmoo_class, cmoo_threshold = get_score(\n",
    "    car_motor_outlier_overlap_performance_svc['y'], car_motor_outlier_overlap_performance_svc['decision_scores'])\n",
    "m_mtrx, m_recall, m_class, m_threshold = get_score(\n",
    "    motor_performance_svc['y'], motor_performance_svc['decision_scores'])\n",
    "c_mtrx, c_recall, c_class, c_threshold = get_score(\n",
    "    car_performance_svc['y'], car_performance_svc['decision_scores'])\n",
    "# 整筆資料\n",
    "f_mtrx, f_recall, f_class, f_threshold = get_score(\n",
    "    full_performance_svc['y'], full_performance_svc['decision_scores'])\n",
    "\n",
    "with open(\"./ModelPerformanceV2/pass_outlier_performance_svc.pkl\", \"rb\") as f:\n",
    "    pass_outlier_performance_svc_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/pass_performance_svc.pkl\", \"rb\") as f:\n",
    "    pass_performance_svc_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/car_motor_outlier_overlap_performance_svc.pkl\", \"rb\") as f:\n",
    "    car_motor_outlier_overlap_performance_svc_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/motor_performance_svc.pkl\", \"rb\") as f:\n",
    "    motor_performance_svc_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/car_performance_svc.pkl\", \"rb\") as f:\n",
    "    car_performance_svc_v2 = pickle.load(f)\n",
    "with open(\"./ModelPerformanceV2/full_performance_svc.pkl\", \"rb\") as f:\n",
    "    full_performance_svc_v2 = pickle.load(f)\n",
    "    \n",
    "# 行人資料V2\n",
    "poo_mtrx_v2, poo_recall_v2, poo_class_v2, poo_threshold_v2 = get_score(\n",
    "    pass_outlier_performance_svc_v2['y'], pass_outlier_performance_svc_v2['decision_scores'], threshold=-5)\n",
    "p_mtrx_v2, p_recall_v2, p_class_v2, p_threshold_v2 = get_score(\n",
    "    pass_performance_svc_v2['y'], pass_performance_svc_v2['decision_scores'])\n",
    "# 駕駛資料V2\n",
    "cmoo_mtrx_v2, cmoo_recall_v2, cmoo_class_v2, cmoo_threshold_v2 = get_score(\n",
    "    car_motor_outlier_overlap_performance_svc_v2['y'], car_motor_outlier_overlap_performance_svc_v2['decision_scores'])\n",
    "m_mtrx_v2, m_recall_v2, m_class_v2, m_threshold_v2 = get_score(\n",
    "    motor_performance_svc_v2['y'], motor_performance_svc_v2['decision_scores'])\n",
    "c_mtrx_v2, c_recall_v2, c_class_v2, c_threshold_v2 = get_score(\n",
    "    car_performance_svc_v2['y'], car_performance_svc_v2['decision_scores'])\n",
    "f_mtrx_v2, f_recall_v2, f_class_v2, f_threshold_v2 = get_score(\n",
    "    full_performance_svc_v2['y'], full_performance_svc_v2['decision_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full version\n",
      "Top recall: 0.7480\n",
      "Bottom recall: 0.6929\n",
      "topology version\n",
      "Top recall: 0.6786\n",
      "Bottom recall: 0.6714\n"
     ]
    }
   ],
   "source": [
    "recall_top_svc_f = f_mtrx_v2[0, 0] / (f_mtrx_v2[0, 0] + f_mtrx_v2[0, 1])\n",
    "recall_bottom_svc_f = f_mtrx_v2[1, 1] / (f_mtrx_v2[1, 1] + f_mtrx_v2[1, 0])\n",
    "\n",
    "print('full version')\n",
    "print(f\"Top recall: {recall_top_svc_f:.4f}\")\n",
    "print(f\"Bottom recall: {recall_bottom_svc_f:.4f}\")\n",
    "\n",
    "mapper_svc = c_mtrx_v2 + m_mtrx + cmoo_mtrx_v2 + p_mtrx + poo_mtrx\n",
    "\n",
    "recall_top_svc_m = mapper_svc[0, 0] / (mapper_svc[0, 0] + mapper_svc[0, 1])\n",
    "recall_bottom_svc_m = mapper_svc[1, 1] / (mapper_svc[1, 1] + mapper_svc[1, 0])\n",
    "\n",
    "print('topology version')\n",
    "print(f\"Top recall: {recall_top_svc_m:.4f}\")\n",
    "print(f\"Bottom recall: {recall_bottom_svc_m:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取總數\n",
    "poo_total_v2 = np.sum(poo_mtrx_v2)\n",
    "p_total_v2 = np.sum(p_mtrx_v2)\n",
    "cmoo_total_v2 = np.sum(cmoo_mtrx_v2)\n",
    "m_total_v2 = np.sum(m_mtrx_v2)\n",
    "c_total_v2 = np.sum(c_mtrx_v2)\n",
    "f_total_v2 = np.sum(f_mtrx_v2)\n",
    "\n",
    "# Example recall values and names for the first version\n",
    "recalls_v1 = [poo_recall, p_recall, cmoo_recall, m_recall, c_recall, f_recall]\n",
    "names_v1 = ['po', 'pass', 'cmoo', 'motor', 'car', 'full']\n",
    "\n",
    "# Recall values for the second version\n",
    "recalls_v2 = [poo_recall_v2, p_recall_v2, cmoo_recall_v2, m_recall_v2, c_recall_v2, f_recall_v2]\n",
    "\n",
    "# Total values for the second version\n",
    "totals_v2 = [poo_total_v2, p_total_v2, cmoo_total_v2, m_total_v2, c_total_v2, f_total_v2]\n",
    "\n",
    "# Create the combined bar chart\n",
    "x = np.arange(len(names_v1))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars1 = ax.barh(x - width/2, recalls_v1, width, label='大類別', color='skyblue')\n",
    "bars2 = ax.barh(x + width/2, recalls_v2, width, label='加上小類別', color='lightcoral')\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        xval = bar.get_width()\n",
    "        plt.text(xval, bar.get_y() + bar.get_height()/2.0, f'{xval:.4f}', ha='left', va='center')\n",
    "\n",
    "# Add total values on the right side of the bars\n",
    "for i, total in enumerate(totals_v2):\n",
    "    plt.text(1.02, i + width/2, f'{total}', ha='left', va='center', fontsize=12, color='black')\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Recall Scores Comparison', fontsize=16)\n",
    "ax.set_xlabel('Recall', fontsize=14)\n",
    "ax.set_ylabel('Models', fontsize=14)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(names_v1)\n",
    "ax.set_xlim(0, 1)  # Ensure the x-axis starts at 0 and ends at 1\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6850393700787402\n",
      "0.7874015748031497\n",
      "0.6\n",
      "0.7928571428571428\n",
      "0.7480314960629921\n",
      "0.6929133858267716\n",
      "0.6785714285714286\n",
      "0.6714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(recall_top_log_f)\n",
    "print(recall_bottom_log_f)\n",
    "print(recall_top_log_m)\n",
    "print(recall_bottom_log_m)\n",
    "print(recall_top_svc_f)\n",
    "print(recall_bottom_svc_f)\n",
    "print(recall_top_svc_m)\n",
    "print(recall_bottom_svc_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels = ['Logistic Full', 'Logistic Mapper', 'SVC Full', 'SVC Mapper']\n",
    "\n",
    "# Top recall values\n",
    "top_recalls = [recall_top_log_f, recall_top_log_m, recall_top_svc_f, recall_top_svc_m]\n",
    "\n",
    "# Bottom recall values\n",
    "bottom_recalls = [recall_bottom_log_f, recall_bottom_log_m, recall_bottom_svc_f, recall_bottom_svc_m]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars1 = ax.bar(x - width/2, top_recalls, width, label='Top Recall', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, bottom_recalls, width, label='Bottom Recall', color='lightcoral')\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Recall Scores Comparison', fontsize=16)\n",
    "ax.set_xlabel('Models', fontsize=14)\n",
    "ax.set_ylabel('Recall', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylim(0, 1)  # Ensure the y-axis starts at 0 and ends at 1\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
