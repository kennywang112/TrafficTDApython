{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\TrafficTDApython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.append('/Users/wangqiqian/Desktop/TrafficTDApython/')\n",
    "sys.path.append('C:/Users/USER/Desktop/TrafficTDApython/')\n",
    "\n",
    "current_dir_path = os.getcwd()\n",
    "current_file_path = os.path.abspath(current_dir_path)\n",
    "current_dir_path = os.path.dirname(current_file_path)\n",
    "parent_dir_path = os.path.dirname(current_dir_path)\n",
    "\n",
    "os.chdir(current_dir_path + '\\\\ultils')\n",
    "# os.chdir(current_dir_path + '/ultils')\n",
    "print(current_dir_path)\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import prince\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from functions import *\n",
    "from chi import *\n",
    "from regressionP import *\n",
    "from models import *\n",
    "from utils_v3 import *\n",
    "from plots import *\n",
    "\n",
    "# os.chdir('/Users/wangqiqian/Desktop/TrafficTDApython/')\n",
    "os.chdir('C:/Users/USER/Desktop/TrafficTDApython/')\n",
    "\n",
    "from Version3.tdamapper.core_old import MapperAlgorithm\n",
    "from Version3.tdamapper.cover import CubicalCover\n",
    "from Version3.tdamapper.clustering import FailSafeClustering\n",
    "from Version3.tdamapper.plot import MapperLayoutInteractive\n",
    "\n",
    "# myfont = FontProperties(fname=r\"/System/Library/Fonts/PingFang.ttc\")\n",
    "# sns.set(style=\"whitegrid\", font=myfont.get_name())\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"Data/NPA_TMA2_1.csv\", low_memory=False)[:-2]\n",
    "data2 = pd.read_csv(\"Data/NPA_TMA2_2.csv\", low_memory=False)[:-2]\n",
    "data3 = pd.read_csv(\"Data/NPA_TMA2_3.csv\", low_memory=False)[:-2]\n",
    "data4 = pd.read_csv(\"Data/NPA_TMA2_4_new.csv\", low_memory=False)[:-2]\n",
    "data5 = pd.read_csv(\"Data/NPA_TMA2_5.csv\", low_memory=False)[:-2]\n",
    "data6 = pd.read_csv(\"Data/NPA_TMA2_6_new.csv\", low_memory=False)[:-2]\n",
    "data7 = pd.read_csv(\"Data/NPA_TMA2_7.csv\", low_memory=False)[:-2]\n",
    "data8 = pd.read_csv(\"Data/NPA_TMA2_8.csv\", low_memory=False)[:-2]\n",
    "data9 = pd.read_csv(\"Data/NPA_TMA2_9.csv\", low_memory=False)[:-2]\n",
    "data10 = pd.read_csv(\"Data/NPA_TMA2_10.csv\", low_memory=False)[:-2]\n",
    "\n",
    "dataA2 = pd.concat([data1, data2, data3, data4, data5, data6, data7, data8, data9, data10], ignore_index=True)\n",
    "# dataA2 = pd.concat([data1, data2, data3, data4, data5], ignore_index=True)\n",
    "\n",
    "dataA1 = pd.read_csv(\"Data/NPA_TMA1_V3.csv\")[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7056\\3268341098.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  car_A1.drop(columns=['當事者行動狀態大類別名稱'], inplace=True)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7056\\3268341098.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  car_A2.drop(columns=['當事者行動狀態大類別名稱'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "select_lst = [\n",
    "    # 月份是為了篩選每個月2萬筆\n",
    "    '發生月份',\n",
    "\n",
    "    '天候名稱', '光線名稱', \n",
    "    '道路類別-第1當事者-名稱', '速限-第1當事者', \n",
    "    '路面狀況-路面鋪裝名稱', '路面狀況-路面狀態名稱', '路面狀況-路面缺陷名稱',\n",
    "    '道路障礙-障礙物名稱', '道路障礙-視距品質名稱', '道路障礙-視距名稱',\n",
    "    '號誌-號誌種類名稱', '號誌-號誌動作名稱',\n",
    "    '車道劃分設施-分道設施-快車道或一般車道間名稱', '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '當事者屬-性-別名稱', '當事者事故發生時年齡',\n",
    "    '保護裝備名稱', '行動電話或電腦或其他相類功能裝置名稱',\n",
    "    '肇事逃逸類別名稱-是否肇逃',\n",
    "    '死亡受傷人數',\n",
    "\n",
    "    # 大類別\n",
    "    '道路型態大類別名稱', '事故位置大類別名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱',\n",
    "    '事故類型及型態大類別名稱', '當事者區分-類別-大類別名稱-車種', '當事者行動狀態大類別名稱',\n",
    "    '車輛撞擊部位大類別名稱-最初', '車輛撞擊部位大類別名稱-其他',\n",
    "\n",
    "    # 兩個欄位只有兩個觀察值不同\n",
    "    '肇因研判大類別名稱-主要',\n",
    "    # '肇因研判大類別名稱-個別',\n",
    "]\n",
    "def preprocess(input_data, select_lst):\n",
    "    # 篩選到第一個順位，因為注重的是單次事故的情況\n",
    "    sample_data = input_data[input_data['當事者順位'] == 1].reset_index(drop=True, inplace=False)\n",
    "    sample_data = sample_data[sample_data['發生月份'] < 11]\n",
    "    dataA = sample_data[select_lst]\n",
    "    \n",
    "    # 將資料分出死亡和受傷，合併到原本的資料後去除多餘的死亡受傷人數\n",
    "    death_injury_data = split_death_injury(dataA['死亡受傷人數'])\n",
    "    dist_df = pd.concat([dataA, death_injury_data], axis=1)\n",
    "    # 補齊缺失值\n",
    "    dist_df[select_lst] = dist_df[select_lst].fillna('未紀錄')\n",
    "\n",
    "    dist_df.drop(columns=['死亡受傷人數'], inplace=True)\n",
    "    \n",
    "    return dist_df\n",
    "\n",
    "dist_dfA1 = preprocess(dataA1, select_lst)\n",
    "dist_dfA2 = preprocess(dataA2, select_lst)\n",
    "\n",
    "dist_dfA2 = dist_dfA2[dist_dfA2['當事者行動狀態大類別名稱'] != '未紀錄']\n",
    "dist_dfA1 = dist_dfA1[dist_dfA1['當事者行動狀態大類別名稱'] != '未紀錄']\n",
    "car_A2 = dist_dfA2[dist_dfA2['當事者行動狀態大類別名稱'] == '車的狀態']\n",
    "car_A1 = dist_dfA1[dist_dfA1['當事者行動狀態大類別名稱'] == '車的狀態']\n",
    "\n",
    "car_A1.drop(columns=['當事者行動狀態大類別名稱'], inplace=True)\n",
    "car_A2.drop(columns=['當事者行動狀態大類別名稱'], inplace=True)\n",
    "\n",
    "car_A2 = car_A2[car_A2['肇因研判大類別名稱-主要'] != '非駕駛者']\n",
    "car_A1 = car_A1[car_A1['肇因研判大類別名稱-主要'] != '非駕駛者']\n",
    "car_A2 = car_A2[car_A2['肇因研判大類別名稱-主要'] != '無(非車輛駕駛人因素)']\n",
    "car_A1 = car_A1[car_A1['肇因研判大類別名稱-主要'] != '無(非車輛駕駛人因素)']\n",
    "# 刪掉原因為類似於他提供不了更多資訊，但會使Mapper更加分散\n",
    "car_A2 = car_A2[car_A2['肇因研判大類別名稱-主要'] != '無(車輛駕駛者因素)']\n",
    "car_A1 = car_A1[car_A1['肇因研判大類別名稱-主要'] != '無(車輛駕駛者因素)']\n",
    "\n",
    "car_A2 = car_A2[car_A2['行動電話或電腦或其他相類功能裝置名稱'] != '未紀錄']\n",
    "car_A1 = car_A1[car_A1['行動電話或電腦或其他相類功能裝置名稱'] != '未紀錄']\n",
    "\n",
    "car_A2 = car_A2[car_A2['車輛撞擊部位大類別名稱-最初'] != '未紀錄']\n",
    "car_A1 = car_A1[car_A1['車輛撞擊部位大類別名稱-最初'] != '未紀錄']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "10080\n",
      "45\n",
      "10309\n",
      "40\n",
      "9163\n",
      "35\n",
      "8018\n",
      "36\n",
      "8247\n",
      "39\n",
      "8934\n",
      "47\n",
      "10767\n",
      "43\n",
      "9851\n",
      "40\n",
      "9163\n",
      "47\n",
      "10767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "sampling_ratio = 0.33  # 下採樣比例\n",
    "\n",
    "# 計算 A1 和 A2 原始數據量比例\n",
    "A1_total = len(car_A1)\n",
    "A2_total = len(car_A2)\n",
    "total_ratio = A1_total / A2_total  # 保留 A1/A2 的比例\n",
    "\n",
    "# 定義函數，按月份進行下採樣\n",
    "def downsample_by_month_simple(A1, A2, sampling_ratio, total_ratio):\n",
    "    A1_downsampled = pd.DataFrame()\n",
    "    A2_downsampled = pd.DataFrame()\n",
    "\n",
    "    months = sorted(set(A1['發生月份']).intersection(A2['發生月份']))  # 確保月份匹配\n",
    "\n",
    "    for month in months:\n",
    "        # 提取該月份的資料\n",
    "        A1_month = A1[A1['發生月份'] == month]\n",
    "        A2_month = A2[A2['發生月份'] == month]\n",
    "\n",
    "        # 計算該月份目標數量\n",
    "        A1_target = int(len(A1_month) * sampling_ratio)\n",
    "        A2_target = int(A1_target / total_ratio)\n",
    "        print(A1_target)\n",
    "        print(A2_target)\n",
    "\n",
    "        # 下採樣\n",
    "        A1_sampled = resample(A1_month, replace=False, n_samples=A1_target, random_state=42)\n",
    "        A2_sampled = resample(A2_month, replace=False, n_samples=A2_target, random_state=42)\n",
    "\n",
    "        # 合併到最終結果\n",
    "        A1_downsampled = pd.concat([A1_downsampled, A1_sampled])\n",
    "        A2_downsampled = pd.concat([A2_downsampled, A2_sampled])\n",
    "\n",
    "    return A1_downsampled.reset_index(drop=True), A2_downsampled.reset_index(drop=True)\n",
    "\n",
    "# 下採樣\n",
    "downsampled_A1, downsampled_A2 = downsample_by_month_simple(car_A1, car_A2, sampling_ratio, total_ratio)\n",
    "# Concat\n",
    "rbind_data = pd.concat([downsampled_A1, downsampled_A2], axis=0, ignore_index=True)\n",
    "rbind_data.drop(columns=['發生月份'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_data: (95715, 146)\n"
     ]
    }
   ],
   "source": [
    "bins_age = [0, 14, 24, 34, 44, 54, 64, 74, float('inf')]\n",
    "labels_age = ['未滿15歲', '15~24', '25~34', '35~44', '45~54', '55~64', '65~74', '75+']\n",
    "\n",
    "rbind_data['當事者事故發生時年齡'] = pd.cut(rbind_data['當事者事故發生時年齡'], bins=bins_age, labels=labels_age, right=False)\n",
    "\n",
    "rbind_data['當事者事故發生時年齡'] = rbind_data['當事者事故發生時年齡'].cat.add_categories('未知')\n",
    "rbind_data['當事者事故發生時年齡'] = rbind_data['當事者事故發生時年齡'].fillna('未知')\n",
    "\n",
    "bins_speed = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, float('inf')]\n",
    "labels_speed = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-110', '110+']\n",
    "\n",
    "rbind_data['速限-第1當事者'] = pd.cut(rbind_data['速限-第1當事者'], bins=bins_speed, labels=labels_speed, right=False)\n",
    "\n",
    "# assert rbind_data.shape[0] == dist_dfA1.shape[0] + dist_dfA2.shape[0]\n",
    "# Dummy\n",
    "rbind_data[\"速限-第1當事者\"] = rbind_data[\"速限-第1當事者\"].astype(str)\n",
    "dummy_data = pd.get_dummies(rbind_data)\n",
    "\n",
    "print('dummy_data:', dummy_data.shape)\n",
    "\n",
    "data_to_map = dummy_data.drop(['死亡', '受傷'], axis=1)\n",
    "mapper_numpy = data_to_map.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, RandomUnderSampler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def logistic_cm_gridsearch(X, y, threshold=0.5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n",
    "\n",
    "    # print(\"Original train class distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "\n",
    "    smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "    enn = EditedNearestNeighbours(n_neighbors=3)\n",
    "    smote_enn = SMOTEENN(smote=smote, enn=enn)\n",
    "    X_resampled_train, y_resampled_train = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "    # print(\"Resampled train class distribution:\", dict(zip(*np.unique(y_resampled_train, return_counts=True))))\n",
    "\n",
    "    min_class_count = min(sum(y_test == 0), sum(y_test == 1))\n",
    "    \n",
    "    rus_test = RandomUnderSampler(sampling_strategy={0: min_class_count, 1: min_class_count}, random_state=42)\n",
    "    X_resampled_test, y_resampled_test = rus_test.fit_resample(X_test, y_test)\n",
    "\n",
    "    # print(\"Resampled test class distribution:\", dict(zip(*np.unique(y_resampled_test, return_counts=True))))\n",
    "\n",
    "    model = LogisticRegression(solver='saga', max_iter=10000)\n",
    "    \n",
    "    parameters = {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.01, 0.1, 1]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, parameters, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled_train, y_resampled_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
    "    \n",
    "    # y_pred = best_model.predict(X_resampled_test)\n",
    "    y_proba = best_model.predict_proba(X_resampled_test)[:, 1]\n",
    "    threshold = threshold\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    print(y_proba)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_resampled_test, y_pred)\n",
    "    accuracy = accuracy_score(y_resampled_test, y_pred)\n",
    "\n",
    "    precision = precision_score(y_resampled_test, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_resampled_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_resampled_test, y_pred, average=None)\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Label': [f'Class_{i}' for i in range(len(precision))],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "    return metrics_df, accuracy, conf_matrix, y_resampled_test, y_proba\n",
    "\n",
    "def linear_svc_cm_gridsearch(X, y, threshold=0.5):\n",
    "    # 分割資料集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n",
    "\n",
    "    # print(\"Original train class distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "\n",
    "    # 資料平衡處理\n",
    "    smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "    enn = EditedNearestNeighbours(n_neighbors=3)\n",
    "    smote_enn = SMOTEENN(smote=smote, enn=enn)\n",
    "    X_resampled_train, y_resampled_train = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "    # print(\"Resampled train class distribution:\", dict(zip(*np.unique(y_resampled_train, return_counts=True))))\n",
    "\n",
    "    # 測試集重新平衡\n",
    "    min_class_count = min(sum(y_test == 0), sum(y_test == 1))\n",
    "    rus_test = RandomUnderSampler(sampling_strategy={0: min_class_count, 1: min_class_count}, random_state=42)\n",
    "    X_resampled_test, y_resampled_test = rus_test.fit_resample(X_test, y_test)\n",
    "\n",
    "    # print(\"Resampled test class distribution:\", dict(zip(*np.unique(y_resampled_test, return_counts=True))))\n",
    "\n",
    "    # 建立線性支持向量機模型\n",
    "    model = LinearSVC(random_state=42, max_iter=100000)\n",
    "\n",
    "    # 超參數範圍\n",
    "    parameters = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'loss': ['hinge', 'squared_hinge']\n",
    "    }\n",
    "\n",
    "    # 使用 GridSearchCV 找最佳參數\n",
    "    grid_search = GridSearchCV(model, parameters, cv=10, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled_train, y_resampled_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
    "\n",
    "    # 預測測試集\n",
    "    # y_pred = best_model.predict(X_resampled_test)\n",
    "    decision_scores = best_model.decision_function(X_resampled_test)\n",
    "    print(decision_scores)\n",
    "    threshold = threshold  # 舉例，根據需要調整\n",
    "    y_pred = (decision_scores >= threshold).astype(int)\n",
    "\n",
    "    # 計算評估指標\n",
    "    conf_matrix = confusion_matrix(y_resampled_test, y_pred)\n",
    "    accuracy = accuracy_score(y_resampled_test, y_pred)\n",
    "\n",
    "    precision = precision_score(y_resampled_test, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_resampled_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_resampled_test, y_pred, average=None)\n",
    "\n",
    "    # 整理成表格\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Label': [f'Class_{i}' for i in range(len(precision))],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "    return metrics_df, accuracy, conf_matrix, y_resampled_test, decision_scores\n",
    "\n",
    "    # 整理成表格\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Label': [f'Class_{i}' for i in range(len(precision))],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "    return metrics_df, accuracy, conf_matrix, y_resampled_test, decision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(input_data):\n",
    "    input_data['y'] = input_data['死亡'].apply(lambda x: 1 if x >= 1 else 0)\n",
    "    \n",
    "    new_input_data = input_data.drop(columns=['受傷', '死亡'], inplace=False)\n",
    "    \n",
    "    X = new_input_data.drop(columns=['y'])\n",
    "    y = new_input_data['y']\n",
    "\n",
    "    return X, y\n",
    "\n",
    "Xfull, yfull = get_train_test_data(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_svc_matrix, full_svc_score, full_svc_cm, full_svc_y_resampled_test, full_svc_decision_scores = linear_svc_cm_gridsearch(Xfull, yfull, threshold=-1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19 101]\n",
      " [ 35  85]] 0.43333333333333335\n"
     ]
    }
   ],
   "source": [
    "print(full_svc_cm, full_svc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料準確率\n",
    "SVC\n",
    "[ 6 10]\n",
    "[ 4 12]\n",
    "Logistic\n",
    "[ 9  7]\n",
    "[ 6 10]\n",
    "駕駛資料準確\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
