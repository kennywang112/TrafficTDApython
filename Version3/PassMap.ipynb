{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\System\\\\Library\\\\Fonts\\\\PingFang.ttc'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# 獲取當前工作目錄\n",
    "current_dir = os.getcwd()\n",
    "version3_path = os.path.join(current_dir, \"TrafficTDApython\", \"Version3\", \"tdamapper\", \"core_old.py\")\n",
    "\n",
    "from utils.models import *\n",
    "from utils.utils_v3 import *\n",
    "from utils.plots import *\n",
    "\n",
    "try:\n",
    "    myfont = FontProperties(fname=r\"/System/Library/Fonts/PingFang.ttc\")\n",
    "    sns.set(style=\"whitegrid\", font=myfont.get_name())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "dataA2 = pd.read_csv(\"../Version3/Data/A2.csv\", low_memory=False)\n",
    "dataA1 = pd.read_csv(\"../Version3/Data/A1.csv\")\n",
    "info = pd.read_csv(\"./Data/PassData/full_info.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_34280\\533796465.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pass_A1.drop(columns=['當事者行動狀態大類別名稱'], inplace=True)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_34280\\533796465.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pass_A2.drop(columns=['當事者行動狀態大類別名稱'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "select_lst = [\n",
    "    '發生月份',\n",
    "    '天候名稱', '光線名稱', \n",
    "    '道路類別-第1當事者-名稱', '速限-第1當事者', \n",
    "    '路面狀況-路面鋪裝名稱', '路面狀況-路面狀態名稱', '路面狀況-路面缺陷名稱',\n",
    "    '道路障礙-障礙物名稱', '道路障礙-視距品質名稱', '道路障礙-視距名稱',\n",
    "    '號誌-號誌種類名稱', '號誌-號誌動作名稱',\n",
    "    '車道劃分設施-分道設施-快車道或一般車道間名稱', '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '當事者屬-性-別名稱', '當事者事故發生時年齡',\n",
    "    '保護裝備名稱', '行動電話或電腦或其他相類功能裝置名稱',\n",
    "    '肇事逃逸類別名稱-是否肇逃',\n",
    "    '死亡受傷人數',\n",
    "\n",
    "    '道路型態大類別名稱', '事故位置大類別名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱',\n",
    "    '事故類型及型態大類別名稱', '當事者區分-類別-大類別名稱-車種', '當事者行動狀態大類別名稱',\n",
    "    '車輛撞擊部位大類別名稱-其他',\n",
    "    '肇因研判大類別名稱-主要',\n",
    "    '經度', '緯度',\n",
    "    \n",
    "    '車輛撞擊部位大類別名稱-最初' \n",
    "]\n",
    "# select_lst = dataA1.columns\n",
    "\n",
    "def preprocess(input_data, select_lst):\n",
    "    # 篩選到第一個順位，因為注重的是單次事故的情況\n",
    "    main_data = input_data[input_data['當事者順位'] == 1].reset_index(drop=True, inplace=False)\n",
    "    sample_data = main_data[main_data['發生月份'] < 11]\n",
    "    selected_data = sample_data[select_lst]\n",
    "    \n",
    "    # 將資料分出死亡和受傷，合併到原本的資料後去除多餘的死亡受傷人數\n",
    "    split_death_injury_data = split_death_injury(selected_data['死亡受傷人數'])\n",
    "    full_data = pd.concat([selected_data, split_death_injury_data], axis=1)\n",
    "\n",
    "    # 補齊缺失值\n",
    "    full_data[select_lst] = full_data[select_lst].fillna('未紀錄')\n",
    "\n",
    "    # 速限範圍\n",
    "    full_data = full_data[(full_data['速限-第1當事者'] < 200) &\n",
    "                      (full_data['當事者事故發生時年齡'] < 100) &\n",
    "                      (full_data['當事者事故發生時年齡'] > 0)]\n",
    "\n",
    "    full_data.drop(columns=['死亡受傷人數'], inplace=True)\n",
    "    \n",
    "    # 篩選非駕駛人的資料\n",
    "    full_data = full_data[full_data['當事者行動狀態大類別名稱'] == '人的狀態']\n",
    "    full_data.drop(columns=['當事者行動狀態大類別名稱'], inplace=True)\n",
    "    # 篩選離群資料(影響MCA的因子得分)\n",
    "    full_data = full_data[(full_data['行動電話或電腦或其他相類功能裝置名稱'] != '未紀錄') &\n",
    "                   (full_data['行動電話或電腦或其他相類功能裝置名稱'] != '不明')]\n",
    "    \n",
    "    return full_data\n",
    "\n",
    "full_dataA1 = preprocess(dataA1, select_lst)\n",
    "full_dataA2 = preprocess(dataA2, select_lst)\n",
    "\n",
    "# Concat\n",
    "rbind_data = pd.concat([full_dataA1, full_dataA2], axis=0, ignore_index=True)\n",
    "rbind_data.drop(columns=['發生月份'], inplace=True)\n",
    "\n",
    "# 處理年齡和速限\n",
    "rbind_data = process_age_speed(rbind_data)\n",
    "rbind_data.drop(['死亡', '受傷'], axis=1, inplace=True)\n",
    "# 唯一值處理\n",
    "columns_to_drop = []\n",
    "for column in rbind_data.columns:\n",
    "    if rbind_data[column].nunique() == 1:  # 檢查唯一值數量是否等於 1\n",
    "        columns_to_drop.append(column)\n",
    "\n",
    "columns_to_drop\n",
    "\n",
    "# 去掉唯一值\n",
    "rbind_data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "dummy_data = pd.get_dummies(rbind_data)\n",
    "print(dummy_data.shape)\n",
    "mapper_numpy = dummy_data.to_numpy()\n",
    "\n",
    "# rbind_data['顯著特徵'] = rbind_data['道路型態子類別名稱'] + ',' + rbind_data['號誌-號誌動作名稱'] + ',' + rbind_data['天候名稱']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_proportion_tables = []\n",
    "\n",
    "for id in info['ids']:\n",
    "    \n",
    "    id_lst = ast.literal_eval(id)\n",
    "    datas = len(id_lst)\n",
    "    normalized_datas = datas / len(dummy_data)\n",
    "    original_data = dummy_data.iloc[id_lst]\n",
    "    proportion_data  = original_data.sum() / len(original_data)\n",
    "    proportion_data['資料數量'] = normalized_datas\n",
    "    proportion_table = proportion_data.to_frame(name='比例').T\n",
    "    all_proportion_tables.append(proportion_table)\n",
    "    \n",
    "final_table = pd.concat(all_proportion_tables, ignore_index=True)\n",
    "\n",
    "columns_to_drop = []\n",
    "for column in final_table.columns:\n",
    "    if final_table[column].nunique() == 1:  # 檢查唯一值數量是否等於 1\n",
    "        columns_to_drop.append(column)\n",
    "        \n",
    "columns_to_drop\n",
    "final_table = final_table.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(input_data, classify=True):\n",
    "    \n",
    "    if classify:\n",
    "        y = input_data['死亡'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    else:\n",
    "        y = input_data['死亡']\n",
    "    new_input_data = input_data.drop(columns=['受傷', '死亡'], inplace=False)\n",
    "    X = new_input_data\n",
    "\n",
    "    return X, y\n",
    "\n",
    "pass_X, pass_y = get_train_test_data(final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def ridge_cm_kfold(X, y, k=5, alpha=1.0):\n",
    "    \"\"\"\n",
    "    使用 Ridge Regression 進行 K-Fold 交叉驗證。\n",
    "\n",
    "    Args:\n",
    "        X (DataFrame): 特徵矩陣。\n",
    "        y (Series): 目標變量。\n",
    "        k (int): K-Fold 的折數，默認為 5。\n",
    "        alpha (float): Ridge 的正則化強度參數，默認為 1.0。\n",
    "\n",
    "    Returns:\n",
    "        np.array: 所有測試集的真實值。\n",
    "        np.array: 所有測試集的預測值。\n",
    "        np.array: 所有測試集的原始索引。\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    original_indices = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Ridge regression model with specified alpha\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Prediction\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Store metrics and results\n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "        original_indices.extend(test_index)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_true_all, y_pred_all)\n",
    "    r2 = r2_score(y_true_all, y_pred_all)\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"R^2 Score: {r2:.4f}\")\n",
    "\n",
    "    return np.array(y_true_all), np.array(y_pred_all), np.array(original_indices)\n",
    "\n",
    "def logistic_cm_kfold(X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    y_true_all = []\n",
    "    y_proba_all = []\n",
    "    original_indices = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Oversampling + undersampling\n",
    "        smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "        enn = EditedNearestNeighbours(n_neighbors=3)\n",
    "        smote_enn = SMOTEENN(smote=smote, enn=enn)\n",
    "        X_resampled_train, y_resampled_train = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Model and GridSearch\n",
    "        model = LogisticRegression(solver='saga', max_iter=10000)\n",
    "        parameters = {\n",
    "            'penalty': ['l2', 'l1'],\n",
    "            'C': [0.01, 0.1, 1, 10]\n",
    "        }\n",
    "        grid_search = GridSearchCV(model, parameters, cv=5, scoring='accuracy', n_jobs=12)\n",
    "        grid_search.fit(X_resampled_train, y_resampled_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        print(f\"Best parameters for this fold: {grid_search.best_params_}\")\n",
    "\n",
    "        # Prediction and probability\n",
    "        y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Store metrics and results\n",
    "        y_true_all.extend(y_test)\n",
    "        y_proba_all.extend(y_proba)\n",
    "        original_indices.extend(test_index)\n",
    "\n",
    "    return np.array(y_true_all), np.array(y_proba_all), np.array(original_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.1714\n",
      "R^2 Score: 0.0856\n"
     ]
    }
   ],
   "source": [
    "pass_y_resampled_test_log, pass_decision_scores_log, pass_indices_log = ridge_cm_kfold(pass_X, pass_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['死亡比例'] = final_table['死亡']\n",
    "info['score'] = pass_decision_scores_log\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: 計算每個索引的總出現次數\n",
    "index_counts = defaultdict(int)\n",
    "\n",
    "for _, row in info.iterrows():\n",
    "    row['ids'] = ast.literal_eval(row['ids'])  # 將字串轉換為列表\n",
    "    for idx in row['ids']:\n",
    "        index_counts[idx] += 1\n",
    "        \n",
    "# Step 2: 根據總次數計算權重\n",
    "weights = defaultdict(float)\n",
    "\n",
    "for _, row in info.iterrows():\n",
    "    row['ids'] = ast.literal_eval(row['ids'])  # 再次解析 ids\n",
    "    for idx in row['ids']:\n",
    "        weights[idx] += row['score'] / index_counts[idx]  # 使用索引的總出現次數作為分母\n",
    "\n",
    "# Step 3: 將結果轉為 DataFrame\n",
    "weights_df = pd.DataFrame(list(weights.items()), columns=['index', 'weight']).sort_values(by='index').reset_index(drop=True)\n",
    "\n",
    "final_data = rbind_data.merge(weights_df, left_index=True, right_on='index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Map/pass_weighted_map.html'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import folium\n",
    "from folium import plugins\n",
    "import matplotlib\n",
    "\n",
    "# Normalize weights for color scaling\n",
    "norm = matplotlib.colors.Normalize(vmin=final_data['weight'].min(), vmax=final_data['weight'].max())\n",
    "cmap = matplotlib.cm.ScalarMappable(norm=norm, cmap='viridis')  # Using 'viridis' colormap\n",
    "\n",
    "# Create a Folium map centered on the first data point\n",
    "m = folium.Map(location=[final_data['緯度'].mean(), final_data['經度'].mean()], zoom_start=12)\n",
    "\n",
    "# Add each point to the map with color based on weight\n",
    "for _, row in final_data.iterrows():\n",
    "    if pd.notna(row['weight']):  # Check if weight is not NaN\n",
    "        color = matplotlib.colors.to_hex(cmap.to_rgba(row['weight']))  # Convert weight to color\n",
    "        folium.CircleMarker(\n",
    "            location=(row['緯度'], row['經度']),\n",
    "            radius=6,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.8,\n",
    "            popup=f\"Weight: {row['weight']:.3f}\"\n",
    "        ).add_to(m)\n",
    "\n",
    "# Save the map as an HTML file\n",
    "map_file_path = \"./Map/pass_weighted_map.html\"\n",
    "m.save(map_file_path)\n",
    "\n",
    "# Provide the link to the user\n",
    "map_file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
