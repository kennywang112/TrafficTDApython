{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from utils.model_score import *\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    myfont = FontProperties(fname=r\"/System/Library/Fonts/PingFang.ttc\")\n",
    "    sns.set(style=\"whitegrid\", font=myfont.get_name())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"./ModelPerformanceSeed\"\n",
    "models = [\"logistic\", \"svc\", \"xgboost\"]\n",
    "variable_names = [\n",
    "    'pass_out_overlap', 'pass_0', 'pass_1',\n",
    "    'car_out_overlap', 'car_0', 'car_1', 'car_2',\n",
    "    'motor_out_overlap', 'motor_0', 'motor_1',\n",
    "    'full_data'\n",
    "]\n",
    "file_paths = []\n",
    "seeds = [40] * 11 + [41] * 11 + [42] * 11 + [43] * 11 + [44] * 11 + [45] * 11 + [46] * 11 + [47] * 11 + [48] * 11 + [49] * 11\n",
    "\n",
    "for model in models:\n",
    "    for seed in range(40, 50):\n",
    "        for var_name in variable_names:\n",
    "            file_paths.append(f\"{prefix}/{seed}/{model}/{var_name}.pkl\")\n",
    "\n",
    "def load_pickle_files(file_paths, variable_names):\n",
    "    data = {}\n",
    "    # 10個seed需要將variable_names重複10次\n",
    "    for file_path, var_name, seed in zip(file_paths, variable_names * 10, seeds):\n",
    "        if seed not in data:\n",
    "            data[seed] = {}  # 只在 seed 不存在時初始化\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # data[seed][var_name] = pickle.load(f)\n",
    "            data[seed][var_name] = pd.read_pickle(file_path)\n",
    "    return data\n",
    "\n",
    "xgb_data = load_pickle_files(file_paths[:110], variable_names)\n",
    "svc_data = load_pickle_files(file_paths[110:220], variable_names)\n",
    "logistic_data = load_pickle_files(file_paths[220:], variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_final_data = {}\n",
    "svc_final_data = {}\n",
    "xgb_final_data = {}\n",
    "\n",
    "for model in models:\n",
    "    for seed in range(40, 50):\n",
    "        for variable_name in variable_names:\n",
    "            if model == \"logistic\":\n",
    "                if seed not in logistic_final_data:\n",
    "                    logistic_final_data[seed] = {}  # 只在 seed 不存在時初始化\n",
    "                logistic_final_data[seed][variable_name] = get_score(\n",
    "                    logistic_data[seed][variable_name]['y'], \n",
    "                    logistic_data[seed][variable_name]['decision_scores']\n",
    "                )\n",
    "\n",
    "            elif model == \"svc\":\n",
    "                if seed not in svc_final_data:\n",
    "                    svc_final_data[seed] = {}\n",
    "                svc_final_data[seed][variable_name] = get_score(\n",
    "                    svc_data[seed][variable_name]['y'], \n",
    "                    svc_data[seed][variable_name]['decision_scores']\n",
    "                )\n",
    "\n",
    "            elif model == \"xgboost\":\n",
    "                if seed not in xgb_final_data:\n",
    "                    xgb_final_data[seed] = {}\n",
    "                xgb_final_data[seed][variable_name] = get_score(\n",
    "                    xgb_data[seed][variable_name]['y'], \n",
    "                    xgb_data[seed][variable_name]['decision_scores']\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_mtrx_log = {}\n",
    "fd_mtrx_log = {}\n",
    "tp_mtrx_svc = {}\n",
    "fd_mtrx_svc = {}\n",
    "tp_mtrx_xgb = {}\n",
    "fd_mtrx_xgb = {}\n",
    "\n",
    "for seed in range(40, 50):\n",
    "    tp_mtrx_log[seed] = np.zeros((2, 2))\n",
    "    fd_mtrx_log[seed] = np.zeros((2, 2))\n",
    "    tp_mtrx_svc[seed] = np.zeros((2, 2))\n",
    "    fd_mtrx_svc[seed] = np.zeros((2, 2))\n",
    "    tp_mtrx_xgb[seed] = np.zeros((2, 2))\n",
    "    fd_mtrx_xgb[seed] = np.zeros((2, 2))\n",
    "    for dataset, values in logistic_final_data[seed].items():\n",
    "        if dataset != \"full_data\":\n",
    "            tp_mtrx_log[seed] += values[0]\n",
    "        elif dataset == \"full_data\":\n",
    "            fd_mtrx_log[seed] += values[0]\n",
    "    for dataset, values in svc_final_data[seed].items():\n",
    "        if dataset != \"full_data\":\n",
    "            tp_mtrx_svc[seed] += values[0]\n",
    "        elif dataset == \"full_data\":\n",
    "            fd_mtrx_svc[seed] += values[0]\n",
    "    for dataset, values in xgb_final_data[seed].items():\n",
    "        if dataset != \"full_data\":\n",
    "            tp_mtrx_xgb[seed] += values[0]\n",
    "        elif dataset == \"full_data\":\n",
    "            fd_mtrx_xgb[seed] += values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(data_dict):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    accuracy_list = []\n",
    "    total_sum = 0\n",
    "\n",
    "    for seed, matrix in data_dict.items():\n",
    "        FP = matrix[0, 1]\n",
    "        TN = matrix[0, 0]\n",
    "        FN = matrix[1, 0]\n",
    "        TP = matrix[1, 1]\n",
    "        \n",
    "        \n",
    "        total_sum += matrix.sum()\n",
    "        \n",
    "        y_true = np.array([1] * int(TP) + [1] * int(FN) + [0] * int(FP) + [0] * int(TN))\n",
    "        y_pred = np.array([1] * int(TP) + [0] * int(FN) + [1] * int(FP) + [0] * int(TN))\n",
    "\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    return {\n",
    "        \"Recall_mean\": np.mean(recall_list), \"Recall_std\": np.std(recall_list, ddof=1),\n",
    "        \"Precision_mean\": np.mean(precision_list), \"Precision_std\": np.std(precision_list, ddof=1),\n",
    "        \"F1-Score_mean\": np.mean(f1_list), \"F1-Score_std\": np.std(f1_list, ddof=1),\n",
    "        \"Accuracy_mean\": np.mean(accuracy_list), \"Accuracy_std\": np.std(accuracy_list, ddof=1),\n",
    "        \"Avg\": total_sum\n",
    "    }\n",
    "\n",
    "def round_metrics(metrics_dict):\n",
    "    return {key: round(value*100, 1) for key, value in metrics_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names and corresponding confusion matrices\n",
    "models = {\n",
    "    \"Topology Log\": (tp_mtrx_log, fd_mtrx_log),\n",
    "    \"Topology SVC\": (tp_mtrx_svc, fd_mtrx_svc),\n",
    "    \"Topology XGB\": (tp_mtrx_xgb, fd_mtrx_xgb)\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "data_rows = []\n",
    "\n",
    "# Compute metrics in a loop\n",
    "for model_name, (tp_mtrx, fd_mtrx) in models.items():\n",
    "    tp_metrics = round_metrics(compute_metrics(tp_mtrx))\n",
    "    fd_metrics = round_metrics(compute_metrics(fd_mtrx))\n",
    "\n",
    "    data_rows.append([model_name, tp_metrics[\"Recall_mean\"], tp_metrics[\"Recall_std\"],\n",
    "                      tp_metrics[\"Precision_mean\"], tp_metrics[\"Precision_std\"],\n",
    "                      tp_metrics[\"F1-Score_mean\"], tp_metrics[\"F1-Score_std\"],\n",
    "                      tp_metrics[\"Accuracy_mean\"], tp_metrics[\"Accuracy_std\"]])\n",
    "\n",
    "    data_rows.append([f\"Full {model_name.split()[1]}\", fd_metrics[\"Recall_mean\"], fd_metrics[\"Recall_std\"],\n",
    "                      fd_metrics[\"Precision_mean\"], fd_metrics[\"Precision_std\"],\n",
    "                      fd_metrics[\"F1-Score_mean\"], fd_metrics[\"F1-Score_std\"],\n",
    "                      fd_metrics[\"Accuracy_mean\"], fd_metrics[\"Accuracy_std\"]])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_rows, columns=[\n",
    "    \"Dataset\", \"Recall_mean\", \"Recall_std\", \"Precision_mean\", \"Precision_std\",\n",
    "    \"F1-Score_mean\", \"F1-Score_std\", \"Accuracy_mean\", \"Accuracy_std\"\n",
    "])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Recall\", \"Precision\", \"F1-Score\", \"Accuracy\"]\n",
    "colors = [\"blue\", \"green\", \"red\", \"purple\"]\n",
    "x = np.arange(len(df[\"Dataset\"]))\n",
    "# Create separate scatter plots for each metric without overlaying\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # Create a 2x2 grid of subplots\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each metric separately\n",
    "for i, metric in enumerate(metrics):\n",
    "    mean_values = df[f\"{metric}_mean\"]\n",
    "    std_values = df[f\"{metric}_std\"]\n",
    "    \n",
    "    axes[i].errorbar(x, mean_values, yerr=std_values, fmt='o', capsize=5, color=colors[i])\n",
    "    \n",
    "    # X-axis labels\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(df[\"Dataset\"])\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[i].set_xlabel(\"Dataset\")\n",
    "    axes[i].set_ylabel(\"Score (%)\")\n",
    "    axes[i].set_title(f\"{metric}\")\n",
    "    \n",
    "    for j, (mean, std) in enumerate(zip(mean_values, std_values)):\n",
    "        axes[i].text(j, mean, f\"{mean:.1f} ± {std:.1f}\", fontsize=9, va='bottom')\n",
    "\n",
    "    # Grid\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 細分群體"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# 針對每個變數計算\n",
    "for var in variable_names:\n",
    "    data_dict = {seed: svc_final_data[seed][var][0] for seed in range(40, 50)}  # 提取混淆矩陣\n",
    "    results[var] = round_metrics(compute_metrics(data_dict))\n",
    "    \n",
    "metrics_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "metrics_df['Avg'] = metrics_df['Avg'].apply(lambda x: x/1000)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Recall\", \"Precision\", \"F1-Score\", \"Accuracy\"]\n",
    "colors = [\"blue\", \"green\", \"red\", \"purple\"]\n",
    "x = np.arange(len(metrics_df.index))\n",
    "# Create separate scatter plots for each metric without overlaying\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # Create a 2x2 grid of subplots\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each metric separately\n",
    "for i, metric in enumerate(metrics):\n",
    "    mean_values = metrics_df[f\"{metric}_mean\"]\n",
    "    std_values = metrics_df[f\"{metric}_std\"]\n",
    "    \n",
    "    # Ensure upper bound does not exceed 100\n",
    "    upper_bounds = np.minimum(mean_values + std_values, 100)\n",
    "\n",
    "    # Error bar plot with capped standard deviation\n",
    "    axes[i].errorbar(x, mean_values, yerr=[std_values, upper_bounds - mean_values], fmt='o', capsize=5, color=colors[i])\n",
    "    \n",
    "    # X-axis labels\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(metrics_df.index, rotation=40)\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[i].set_xlabel(\"Dataset\")\n",
    "    axes[i].set_ylabel(\"Score (%)\")\n",
    "    axes[i].set_title(f\"{metric}\")\n",
    "    \n",
    "    for j, (mean, std, upper) in enumerate(zip(mean_values, std_values, upper_bounds)):\n",
    "        display_std = upper - mean  # Adjusted std to prevent exceeding 100\n",
    "        axes[i].text(j, mean, f\"{mean:.1f} ± {display_std:.1f}\", fontsize=7, va='bottom')\n",
    "\n",
    "    # Grid\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
