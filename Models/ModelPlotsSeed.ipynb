{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\System\\\\Library\\\\Fonts\\\\PingFang.ttc'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from utils.model_score import *\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    myfont = FontProperties(fname=r\"/System/Library/Fonts/PingFang.ttc\")\n",
    "    sns.set(style=\"whitegrid\", font=myfont.get_name())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"./ModelPerformanceSeed\"\n",
    "models = [\"logistic\", \"svc\", \"xgboost\"]\n",
    "variable_names = [\n",
    "    'pass_out_overlap', 'pass_0', 'pass_1',\n",
    "    'car_out_overlap', 'car_0', 'car_1', 'car_2',\n",
    "    'motor_out_overlap', 'motor_0', 'motor_1',\n",
    "    'full_data'\n",
    "]\n",
    "file_paths = []\n",
    "seeds = [40] * 11 + [41] * 11 + [42] * 11 + [43] * 11 + [44] * 11 + [45] * 11 + [46] * 11 + [47] * 11 + [48] * 11 + [49] * 11\n",
    "\n",
    "for model in models:\n",
    "    for seed in range(40, 50):\n",
    "        for var_name in variable_names:\n",
    "            file_paths.append(f\"{prefix}/{seed}/{model}/{var_name}.pkl\")\n",
    "\n",
    "def load_pickle_files(file_paths, variable_names):\n",
    "    data = {}\n",
    "    # 10個seed需要將variable_names重複10次\n",
    "    for file_path, var_name, seed in zip(file_paths, variable_names * 10, seeds):\n",
    "        if seed not in data:\n",
    "            data[seed] = {}  # 只在 seed 不存在時初始化\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data[seed][var_name] = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "xgb_data = load_pickle_files(file_paths[:110], variable_names)\n",
    "svc_data = load_pickle_files(file_paths[110:220], variable_names)\n",
    "logistic_data = load_pickle_files(file_paths[220:], variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_final_data = {}\n",
    "svc_final_data = {}\n",
    "xgb_final_data = {}\n",
    "\n",
    "for model in models:\n",
    "    for seed in range(40, 50):\n",
    "        for variable_name in variable_names:\n",
    "            if model == \"logistic\":\n",
    "                if seed not in logistic_final_data:\n",
    "                    logistic_final_data[seed] = {}  # 只在 seed 不存在時初始化\n",
    "                logistic_final_data[seed][variable_name] = get_score(\n",
    "                    logistic_data[seed][variable_name]['y'], \n",
    "                    logistic_data[seed][variable_name]['decision_scores']\n",
    "                )\n",
    "\n",
    "            elif model == \"svc\":\n",
    "                if seed not in svc_final_data:\n",
    "                    svc_final_data[seed] = {}\n",
    "                svc_final_data[seed][variable_name] = get_score(\n",
    "                    svc_data[seed][variable_name]['y'], \n",
    "                    svc_data[seed][variable_name]['decision_scores']\n",
    "                )\n",
    "\n",
    "            elif model == \"xgboost\":\n",
    "                if seed not in xgb_final_data:\n",
    "                    xgb_final_data[seed] = {}\n",
    "                xgb_final_data[seed][variable_name] = get_score(\n",
    "                    xgb_data[seed][variable_name]['y'], \n",
    "                    xgb_data[seed][variable_name]['decision_scores']\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_mtrx_log = {}\n",
    "fd_mtrx_log = {}\n",
    "tp_mtrx_svc = {}\n",
    "fd_mtrx_svc = {}\n",
    "tp_mtrx_xgb = {}\n",
    "fd_mtrx_xgb = {}\n",
    "\n",
    "for seed in range(40, 50):\n",
    "    tp_mtrx_log[seed] = np.zeros((2, 2))\n",
    "    fd_mtrx_log[seed] = np.zeros((2, 2))\n",
    "    tp_mtrx_svc[seed] = np.zeros((2, 2))\n",
    "    fd_mtrx_svc[seed] = np.zeros((2, 2))\n",
    "    tp_mtrx_xgb[seed] = np.zeros((2, 2))\n",
    "    fd_mtrx_xgb[seed] = np.zeros((2, 2))\n",
    "    for dataset, values in logistic_final_data[seed].items():\n",
    "        if dataset != \"full_data\":\n",
    "            tp_mtrx_log[seed] += values[0]\n",
    "        elif dataset == \"full_data\":\n",
    "            fd_mtrx_log[seed] += values[0]\n",
    "    for dataset, values in svc_final_data[seed].items():\n",
    "        if dataset != \"full_data\":\n",
    "            tp_mtrx_svc[seed] += values[0]\n",
    "        elif dataset == \"full_data\":\n",
    "            fd_mtrx_svc[seed] += values[0]\n",
    "    for dataset, values in xgb_final_data[seed].items():\n",
    "        if dataset != \"full_data\":\n",
    "            tp_mtrx_xgb[seed] += values[0]\n",
    "        elif dataset == \"full_data\":\n",
    "            fd_mtrx_xgb[seed] += values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(data_dict):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    accuracy_list = []\n",
    "    total_sum = 0\n",
    "\n",
    "    for seed, matrix in data_dict.items():\n",
    "        FP = matrix[0, 1]\n",
    "        TN = matrix[0, 0]\n",
    "        FN = matrix[1, 0]\n",
    "        TP = matrix[1, 1]\n",
    "        \n",
    "        \n",
    "        total_sum += matrix.sum()\n",
    "        \n",
    "        y_true = np.array([1] * int(TP) + [1] * int(FN) + [0] * int(FP) + [0] * int(TN))\n",
    "        y_pred = np.array([1] * int(TP) + [0] * int(FN) + [1] * int(FP) + [0] * int(TN))\n",
    "\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    return {\n",
    "        \"Recall_mean\": np.mean(recall_list), \"Recall_std\": np.std(recall_list, ddof=1),\n",
    "        \"Precision_mean\": np.mean(precision_list), \"Precision_std\": np.std(precision_list, ddof=1),\n",
    "        \"F1-Score_mean\": np.mean(f1_list), \"F1-Score_std\": np.std(f1_list, ddof=1),\n",
    "        \"Accuracy_mean\": np.mean(accuracy_list), \"Accuracy_std\": np.std(accuracy_list, ddof=1),\n",
    "        \"Avg\": total_sum\n",
    "    }\n",
    "\n",
    "def round_metrics(metrics_dict):\n",
    "    return {key: round(value*100, 1) for key, value in metrics_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Recall_mean</th>\n",
       "      <th>Recall_std</th>\n",
       "      <th>Precision_mean</th>\n",
       "      <th>Precision_std</th>\n",
       "      <th>F1-Score_mean</th>\n",
       "      <th>F1-Score_std</th>\n",
       "      <th>Accuracy_mean</th>\n",
       "      <th>Accuracy_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topology Log</td>\n",
       "      <td>64.3</td>\n",
       "      <td>5.6</td>\n",
       "      <td>71.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>67.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>69.1</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Log</td>\n",
       "      <td>61.4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>59.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>59.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>59.1</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topology SVC</td>\n",
       "      <td>64.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>75.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>69.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Full SVC</td>\n",
       "      <td>55.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>72.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>66.9</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topology XGB</td>\n",
       "      <td>63.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>76.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>71.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Full XGB</td>\n",
       "      <td>56.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>71.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>62.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>66.9</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset  Recall_mean  Recall_std  Precision_mean  Precision_std  \\\n",
       "0  Topology Log         64.3         5.6            71.4            2.8   \n",
       "1      Full Log         61.4        14.7            59.4            3.0   \n",
       "2  Topology SVC         64.6         3.7            75.3            3.4   \n",
       "3      Full SVC         55.0         9.9            72.9            3.4   \n",
       "4  Topology XGB         63.0         5.7            76.8            3.8   \n",
       "5      Full XGB         56.4         9.6            71.7            2.4   \n",
       "\n",
       "   F1-Score_mean  F1-Score_std  Accuracy_mean  Accuracy_std  \n",
       "0           67.5           2.3           69.1           1.2  \n",
       "1           59.2           6.6           59.1           1.2  \n",
       "2           69.4           1.1           71.5           1.0  \n",
       "3           62.0           5.2           66.9           1.9  \n",
       "4           68.9           2.3           71.7           1.0  \n",
       "5           62.6           5.0           66.9           1.9  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model names and corresponding confusion matrices\n",
    "models = {\n",
    "    \"Topology Log\": (tp_mtrx_log, fd_mtrx_log),\n",
    "    \"Topology SVC\": (tp_mtrx_svc, fd_mtrx_svc),\n",
    "    \"Topology XGB\": (tp_mtrx_xgb, fd_mtrx_xgb)\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "data_rows = []\n",
    "\n",
    "# Compute metrics in a loop\n",
    "for model_name, (tp_mtrx, fd_mtrx) in models.items():\n",
    "    tp_metrics = round_metrics(compute_metrics(tp_mtrx))\n",
    "    fd_metrics = round_metrics(compute_metrics(fd_mtrx))\n",
    "\n",
    "    data_rows.append([model_name, tp_metrics[\"Recall_mean\"], tp_metrics[\"Recall_std\"],\n",
    "                      tp_metrics[\"Precision_mean\"], tp_metrics[\"Precision_std\"],\n",
    "                      tp_metrics[\"F1-Score_mean\"], tp_metrics[\"F1-Score_std\"],\n",
    "                      tp_metrics[\"Accuracy_mean\"], tp_metrics[\"Accuracy_std\"]])\n",
    "\n",
    "    data_rows.append([f\"Full {model_name.split()[1]}\", fd_metrics[\"Recall_mean\"], fd_metrics[\"Recall_std\"],\n",
    "                      fd_metrics[\"Precision_mean\"], fd_metrics[\"Precision_std\"],\n",
    "                      fd_metrics[\"F1-Score_mean\"], fd_metrics[\"F1-Score_std\"],\n",
    "                      fd_metrics[\"Accuracy_mean\"], fd_metrics[\"Accuracy_std\"]])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_rows, columns=[\n",
    "    \"Dataset\", \"Recall_mean\", \"Recall_std\", \"Precision_mean\", \"Precision_std\",\n",
    "    \"F1-Score_mean\", \"F1-Score_std\", \"Accuracy_mean\", \"Accuracy_std\"\n",
    "])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Recall\", \"Precision\", \"F1-Score\", \"Accuracy\"]\n",
    "colors = [\"blue\", \"green\", \"red\", \"purple\"]\n",
    "x = np.arange(len(df[\"Dataset\"]))\n",
    "# Create separate scatter plots for each metric without overlaying\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # Create a 2x2 grid of subplots\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each metric separately\n",
    "for i, metric in enumerate(metrics):\n",
    "    mean_values = df[f\"{metric}_mean\"]\n",
    "    std_values = df[f\"{metric}_std\"]\n",
    "    \n",
    "    axes[i].errorbar(x, mean_values, yerr=std_values, fmt='o', capsize=5, color=colors[i])\n",
    "    \n",
    "    # X-axis labels\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(df[\"Dataset\"])\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[i].set_xlabel(\"Dataset\")\n",
    "    axes[i].set_ylabel(\"Score (%)\")\n",
    "    axes[i].set_title(f\"{metric}\")\n",
    "    \n",
    "    for j, (mean, std) in enumerate(zip(mean_values, std_values)):\n",
    "        axes[i].text(j, mean, f\"{mean:.1f} ± {std:.1f}\", fontsize=9, va='bottom')\n",
    "\n",
    "    # Grid\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 細分群體"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall_mean</th>\n",
       "      <th>Recall_std</th>\n",
       "      <th>Precision_mean</th>\n",
       "      <th>Precision_std</th>\n",
       "      <th>F1-Score_mean</th>\n",
       "      <th>F1-Score_std</th>\n",
       "      <th>Accuracy_mean</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pass_out_overlap</th>\n",
       "      <td>80.8</td>\n",
       "      <td>13.6</td>\n",
       "      <td>92.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>84.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>85.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pass_0</th>\n",
       "      <td>85.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>83.8</td>\n",
       "      <td>18.8</td>\n",
       "      <td>78.7</td>\n",
       "      <td>22.6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pass_1</th>\n",
       "      <td>79.2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>77.2</td>\n",
       "      <td>12.3</td>\n",
       "      <td>77.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>76.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_out_overlap</th>\n",
       "      <td>51.3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>81.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>60.9</td>\n",
       "      <td>8.5</td>\n",
       "      <td>68.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_0</th>\n",
       "      <td>65.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>7.6</td>\n",
       "      <td>67.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>68.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>165.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_1</th>\n",
       "      <td>64.6</td>\n",
       "      <td>10.4</td>\n",
       "      <td>78.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>70.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_2</th>\n",
       "      <td>55.5</td>\n",
       "      <td>24.6</td>\n",
       "      <td>84.6</td>\n",
       "      <td>15.8</td>\n",
       "      <td>61.8</td>\n",
       "      <td>19.2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motor_out_overlap</th>\n",
       "      <td>61.7</td>\n",
       "      <td>24.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.7</td>\n",
       "      <td>18.5</td>\n",
       "      <td>77.3</td>\n",
       "      <td>11.5</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motor_0</th>\n",
       "      <td>67.2</td>\n",
       "      <td>6.2</td>\n",
       "      <td>79.7</td>\n",
       "      <td>5.8</td>\n",
       "      <td>72.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>74.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>187.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motor_1</th>\n",
       "      <td>56.9</td>\n",
       "      <td>18.5</td>\n",
       "      <td>74.2</td>\n",
       "      <td>10.7</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>52.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_data</th>\n",
       "      <td>55.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>72.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>66.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>642.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Recall_mean  Recall_std  Precision_mean  Precision_std  \\\n",
       "pass_out_overlap          80.8        13.6            92.3           13.5   \n",
       "pass_0                    85.7        30.7            83.8           18.8   \n",
       "pass_1                    79.2        12.5            77.2           12.3   \n",
       "car_out_overlap           51.3        13.3            81.5           11.7   \n",
       "car_0                     65.9        11.0            70.5            7.6   \n",
       "car_1                     64.6        10.4            78.2            5.5   \n",
       "car_2                     55.5        24.6            84.6           15.8   \n",
       "motor_out_overlap         61.7        24.0            93.0           12.0   \n",
       "motor_0                   67.2         6.2            79.7            5.8   \n",
       "motor_1                   56.9        18.5            74.2           10.7   \n",
       "full_data                 55.0         9.9            72.9            3.4   \n",
       "\n",
       "                   F1-Score_mean  F1-Score_std  Accuracy_mean  Accuracy_std  \\\n",
       "pass_out_overlap            84.9           8.8           85.5           9.5   \n",
       "pass_0                      78.7          22.6           80.0          16.6   \n",
       "pass_1                      77.1           8.5           76.6           8.9   \n",
       "car_out_overlap             60.9           8.5           68.4           2.7   \n",
       "car_0                       67.1           3.7           68.1           3.0   \n",
       "car_1                       70.2           6.1           73.0           4.0   \n",
       "car_2                       61.8          19.2           70.0           9.3   \n",
       "motor_out_overlap           70.7          18.5           77.3          11.5   \n",
       "motor_0                     72.6           3.4           74.7           3.2   \n",
       "motor_1                     62.0          10.0           67.1           4.9   \n",
       "full_data                   62.0           5.2           66.9           1.9   \n",
       "\n",
       "                     Avg  \n",
       "pass_out_overlap     9.0  \n",
       "pass_0               7.6  \n",
       "pass_1              21.8  \n",
       "car_out_overlap     48.0  \n",
       "car_0              165.6  \n",
       "car_1              100.0  \n",
       "car_2               21.0  \n",
       "motor_out_overlap    9.2  \n",
       "motor_0            187.6  \n",
       "motor_1             52.4  \n",
       "full_data          642.6  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# 針對每個變數計算\n",
    "for var in variable_names:\n",
    "    data_dict = {seed: svc_final_data[seed][var][0] for seed in range(40, 50)}  # 提取混淆矩陣\n",
    "    results[var] = round_metrics(compute_metrics(data_dict))\n",
    "    \n",
    "metrics_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "metrics_df['Avg'] = metrics_df['Avg'].apply(lambda x: x/1000)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Recall\", \"Precision\", \"F1-Score\", \"Accuracy\"]\n",
    "colors = [\"blue\", \"green\", \"red\", \"purple\"]\n",
    "x = np.arange(len(metrics_df.index))\n",
    "# Create separate scatter plots for each metric without overlaying\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # Create a 2x2 grid of subplots\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each metric separately\n",
    "for i, metric in enumerate(metrics):\n",
    "    mean_values = metrics_df[f\"{metric}_mean\"]\n",
    "    std_values = metrics_df[f\"{metric}_std\"]\n",
    "    \n",
    "    # Ensure upper bound does not exceed 100\n",
    "    upper_bounds = np.minimum(mean_values + std_values, 100)\n",
    "\n",
    "    # Error bar plot with capped standard deviation\n",
    "    axes[i].errorbar(x, mean_values, yerr=[std_values, upper_bounds - mean_values], fmt='o', capsize=5, color=colors[i])\n",
    "    \n",
    "    # X-axis labels\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(metrics_df.index, rotation=40)\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[i].set_xlabel(\"Dataset\")\n",
    "    axes[i].set_ylabel(\"Score (%)\")\n",
    "    axes[i].set_title(f\"{metric}\")\n",
    "    \n",
    "    for j, (mean, std, upper) in enumerate(zip(mean_values, std_values, upper_bounds)):\n",
    "        display_std = upper - mean  # Adjusted std to prevent exceeding 100\n",
    "        axes[i].text(j, mean, f\"{mean:.1f} ± {display_std:.1f}\", fontsize=7, va='bottom')\n",
    "\n",
    "    # Grid\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
