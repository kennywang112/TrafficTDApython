{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77cc55ff-3f53-45f7-b598-d35f5860e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from tdamapper.core import MapperAlgorithm\n",
    "from tdamapper.cover import CubicalCover\n",
    "from tdamapper.plot import MapperLayoutInteractive\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from tdamapper.clustering import FailSafeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_argmin_min, silhouette_score\n",
    "\n",
    "from functions import *\n",
    "from chi import *\n",
    "from regressionP import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32227d39-9fac-46f6-8c56-eee9f665c075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(input_data, sample = 592):\n",
    "    sample_data = input_data[input_data['當事者順位'] == 1].reset_index(drop=True, inplace=False).sample(sample).reset_index(drop=True)\n",
    "    dataA = sample_data[select_lst]\n",
    "    \n",
    "    death_injury_data = split_death_injury(dataA['死亡受傷人數'])\n",
    "    dist_df = pd.concat([dataA, death_injury_data], axis=1)\n",
    "    dist_df.drop(columns=['死亡受傷人數'], inplace=True)\n",
    "    \n",
    "    return dist_df, sample_data\n",
    "\n",
    "def get_calinski_from_db(input_data, eps): \n",
    "    X = input_data.iloc[:, 3:6]\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=10).fit(X)\n",
    "    labels = db.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    input_data['label'] = labels\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "    if len(set(labels)) != 1:\n",
    "        score = metrics.calinski_harabasz_score(X, labels)\n",
    "        silhouette_score_value = silhouette_score(X, labels)\n",
    "    else:\n",
    "        score = -1\n",
    "        silhouette_score_value = -1\n",
    "        \n",
    "    return score, input_data, db, labels, n_clusters_, silhouette_score_value, unique_labels, colors\n",
    "\n",
    "def latlon_to_xyz(lat, lon):\n",
    "    lat_rad = np.radians(lat)\n",
    "    lon_rad = np.radians(lon)\n",
    "    x = np.cos(lat_rad) * np.cos(lon_rad)\n",
    "    y = np.cos(lat_rad) * np.sin(lon_rad)\n",
    "    z = np.sin(lat_rad)\n",
    "    return np.vstack((x, y, z)).T\n",
    "\n",
    "# 計算球面距離\n",
    "def spherical_dist(pos1, pos2, radius=6371):\n",
    "    cos_angle = np.dot(pos1, pos2.T)\n",
    "    angle = np.arccos(np.clip(cos_angle, -1, 1))\n",
    "    return radius * angle\n",
    "\n",
    "def calculate_distances(df, scenic_df, colname, distance_threshold=500):\n",
    "    # 將經緯度轉換成球體的三維坐標\n",
    "    df_xyz = latlon_to_xyz(df['緯度'], df['經度'])\n",
    "    scenic_xyz = latlon_to_xyz(scenic_df['緯度'], scenic_df['經度'])\n",
    "\n",
    "    # 計算所有配對之間的球面距離，並轉換為米\n",
    "    distances = spherical_dist(df_xyz, scenic_xyz) * 1000\n",
    "\n",
    "    # 檢查哪些距離小於設定的threshold\n",
    "    distances_less_than_threshold = (distances < distance_threshold)\n",
    "\n",
    "    # 計算每一行小於閾值的點數量\n",
    "    df[colname] = distances_less_than_threshold.sum(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5606481a-a9a8-4ae8-8179-b2ea7f4af9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"./Data/NPA_TMA2_1.csv\", low_memory=False)[:-2]\n",
    "data2 = pd.read_csv(\"./Data/NPA_TMA2_2.csv\", low_memory=False)[:-2]\n",
    "data3 = pd.read_csv(\"./Data/NPA_TMA2_3.csv\", low_memory=False)[:-2]\n",
    "data4 = pd.read_csv(\"./Data/NPA_TMA2_4.csv\", low_memory=False)[:-2]\n",
    "dataA2 = pd.concat([data1, data2, data3, data4], ignore_index=True)\n",
    "dataA1 = pd.read_csv(\"./Data/NPA_TMA1.csv\")[:-2]\n",
    "# Qmap\n",
    "school_data = pd.read_csv('CalculatedData/coord/school_data.csv')\n",
    "train_data = pd.read_csv('CalculatedData/coord/train_data.csv')\n",
    "post_data = pd.read_csv('CalculatedData/coord/post_data.csv')\n",
    "museum_data = pd.read_csv('CalculatedData/coord/museum_data.csv')\n",
    "guesthouse_data = pd.read_csv('CalculatedData/coord/guesthouse_data.csv')\n",
    "themepark_data = pd.read_csv('CalculatedData/coord/themepark_data.csv')\n",
    "supermarket_data = pd.read_csv('CalculatedData/coord/supermarket_data.csv')\n",
    "coffee_data = pd.read_csv('CalculatedData/coord/coffee_data.csv')\n",
    "fastfood_data = pd.read_csv('CalculatedData/coord/fastfood_data.csv')\n",
    "rtc_data = pd.read_csv('CalculatedData/coord/rtc_data.csv')\n",
    "thsrc_data = pd.read_csv('CalculatedData/coord/thsrc_data.csv')\n",
    "baseball_data = pd.read_csv('CalculatedData/coord/baseball_data.csv')\n",
    "night_data = pd.read_csv('CalculatedData/coord/night_data.csv')\n",
    "port_data = pd.read_csv('CalculatedData/coord/port_data.csv')\n",
    "library_data = pd.read_csv('CalculatedData/coord/library_data.csv')\n",
    "bank_data = pd.read_csv('CalculatedData/coord/bank_data.csv')\n",
    "hospital_data = pd.read_csv('CalculatedData/coord/hospital_data.csv')\n",
    "temple_data = pd.read_csv('CalculatedData/coord/temple_data.csv')\n",
    "police_data = pd.read_csv('CalculatedData/coord/police_data.csv')\n",
    "gas_data = pd.read_csv('CalculatedData/coord/gas_data.csv')\n",
    "town_office_data = pd.read_csv('CalculatedData/coord/town_office_data.csv')\n",
    "hr_office_data = pd.read_csv('CalculatedData/coord/hr_office_data.csv')\n",
    "mv_data = pd.read_csv('CalculatedData/coord/mv_data.csv')\n",
    "ntb_data = pd.read_csv('CalculatedData/coord/ntb_data.csv')\n",
    "# 開放平台\n",
    "scenic = pd.read_csv(\"./Data/Scenic_Spot_C_f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2bb92fa-7ec3-40e5-aa9f-452a061346c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to select\n",
    "select_lst = [\n",
    "    '天候名稱', \n",
    "    '路面狀況-路面狀態名稱',\n",
    "    '肇因研判大類別名稱-主要', '當事者屬-性-別名稱', '當事者事故發生時年齡',\n",
    "    '車輛撞擊部位大類別名稱-最初',\n",
    "    '光線名稱',\n",
    "    '道路類別-第1當事者-名稱',\n",
    "    '速限-第1當事者',\n",
    "    '道路型態大類別名稱',\n",
    "    '事故位置大類別名稱', \n",
    "    '號誌-號誌種類名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱', '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '事故類型及型態大類別名稱',\n",
    "    '死亡受傷人數',\n",
    "    '經度', '緯度',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac9b4ad5-1834-4c18-b5f6-3992299aafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dfA1 = preprocess(dataA1, sample = 592)\n",
    "dist_dfA2 = preprocess(dataA2, sample = 5920)\n",
    "\n",
    "data_frames = {\n",
    "    '學校': school_data,\n",
    "    # '火車站': train_data,\n",
    "    # '郵局': post_data,\n",
    "    # '博物館': museum_data,\n",
    "    # '民宿': guesthouse_data,\n",
    "    # '遊樂園': themepark_data,\n",
    "    # '大賣場': supermarket_data,\n",
    "    # '咖啡店': coffee_data,\n",
    "    # '速食店': fastfood_data,\n",
    "    # '捷運車站': rtc_data,\n",
    "    # '高鐵車站': thsrc_data,\n",
    "    # '棒球場': baseball_data,\n",
    "    # '夜市': night_data,\n",
    "    # '漁港': port_data,\n",
    "    # '圖書館': library_data,\n",
    "    # '銀行': bank_data,\n",
    "    # '醫院': hospital_data,\n",
    "    # '寺廟': temple_data,\n",
    "    # '警察局': police_data,\n",
    "    # '加油站': gas_data,\n",
    "    # '公所': town_office_data,\n",
    "    # '戶政事務所': hr_office_data,\n",
    "    # '監理站': mv_data,\n",
    "    # '國稅局': ntb_data,\n",
    "}\n",
    "for colname, data_frame in data_frames.items():\n",
    "    scenic_dfA1 = calculate_distances(dist_dfA1[0], data_frame, colname=colname)\n",
    "    scenic_dfA2 = calculate_distances(dist_dfA2[0], data_frame, colname=colname)\n",
    "    \n",
    "rbind_data = pd.concat([scenic_dfA1, scenic_dfA2], axis=0, ignore_index=True)\n",
    "\n",
    "for col in data_frames.keys():\n",
    "    rbind_data.loc[rbind_data[col] > 1, col] = 2\n",
    "rbind_data.loc[rbind_data['受傷'] > 1, '受傷'] = 2\n",
    "rbind_data['速限-第1當事者'] = rbind_data['速限-第1當事者'].apply(lambda x: 1 if x > 60 else 0)\n",
    "rbind_data = process_age(rbind_data)\n",
    "\n",
    "dist_df = process_data(rbind_data)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "full_dist = pd.DataFrame(scaler.fit_transform(dist_df), columns = dist_df.columns)\n",
    "X1 = full_dist.drop(['受傷', '死亡', '經度', '緯度'], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55637584-f062-4da0-83db-fdd4325737cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ratio(input_data, components) :\n",
    "    best_comp = {}\n",
    "    for comp in range(1,components+1):   \n",
    "        pca = PCA(comp).fit(input_data)\n",
    "        \n",
    "        best_comp[comp] = pca.explained_variance_ratio_.sum()\n",
    "        \n",
    "    max_comp = max(best_comp, key=best_comp.get)  # 使用 key=best_comp.get 找到最大值的鍵\n",
    "    print(\"最佳成分數：\", max_comp)\n",
    "    print(\"解釋方差比率累計值：\", best_comp[max_comp])\n",
    "\n",
    "# lens1 = find_ratio(X1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ca9382-c45c-4223-a925-aa57393004fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lens1 = PCA(10).fit_transform(X1)\n",
    "\n",
    "mapper_algo1 = MapperAlgorithm(\n",
    "    cover = CubicalCover(\n",
    "        n_intervals = 3,\n",
    "        overlap_frac = 0.3\n",
    "    ),\n",
    "    clustering = FailSafeClustering(\n",
    "        clustering = AgglomerativeClustering(3, affinity='euclidean', linkage='ward'),\n",
    "        verbose = False)\n",
    ")\n",
    "\n",
    "mapper_graph1 = mapper_algo1.fit_transform(X1, lens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e967cd13-c7d5-49b2-9d3c-208ff95b41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper_plot1 = MapperLayoutInteractive(\n",
    "#     mapper_graph1,\n",
    "#     colors = dist_df[['路面狀況-路面狀態名稱']].to_numpy(),\n",
    "#     cmap = 'jet',\n",
    "#     # agg = np.nanmean,\n",
    "#     agg = most_frequent_nonan,\n",
    "#     dim = 3,\n",
    "#     iterations = 30,\n",
    "#     seed = 5,\n",
    "#     width = 800,\n",
    "#     height = 500)\n",
    "\n",
    "# fig_mean1 = mapper_plot1.plot()\n",
    "# fig_mean1.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ff84fb-ef16-43e4-b4de-669a5b1e2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vars(mapper_plot1._MapperLayoutInteractive__fig)['_data_objs'][1]['x']\n",
    "y = vars(mapper_plot1._MapperLayoutInteractive__fig)['_data_objs'][1]['y']\n",
    "z = vars(mapper_plot1._MapperLayoutInteractive__fig)['_data_objs'][1]['z']\n",
    "\n",
    "threeDimData = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "\n",
    "import re\n",
    "data_tuple = vars(mapper_plot1._MapperLayoutInteractive__fig)['_data_objs'][1]['text']\n",
    "\n",
    "data = []\n",
    "for item in data_tuple:\n",
    "    color = int(re.search(r'color: (\\d+)', item).group(1))\n",
    "    node = int(re.search(r'node: (\\d+)', item).group(1))\n",
    "    size = int(re.search(r'size: (\\d+)', item).group(1))\n",
    "    data.append({'color': color, 'node': node, 'size': size})\n",
    "component_info = pd.DataFrame(data)\n",
    "\n",
    "full_info = pd.concat([component_info, threeDimData], axis=1)\n",
    "\n",
    "mp_content_origin = vars(mapper_plot1._MapperLayoutInteractive__graph)['_node']\n",
    "\n",
    "mp_content = pd.DataFrame.from_dict(mp_content_origin, orient='index')\n",
    "mp_content.reset_index(inplace=True)\n",
    "mp_content.rename(columns={'index': 'node'}, inplace=True)\n",
    "\n",
    "full_info = pd.merge(full_info, mp_content, on=['node', 'size'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cc5e535-4ee9-4ad3-b4cc-eea147528b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calinski_data = get_calinski_from_db(full_info, 0.025)\n",
    "# labels = calinski_data[3]\n",
    "# db = calinski_data[2]\n",
    "# n_clusters_ = calinski_data[4]\n",
    "\n",
    "# unique_labels = set(labels)\n",
    "# core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "# core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# def matplotlib_to_plotly(cmap, alpha=1):\n",
    "#     \"\"\"rgba\"\"\"\n",
    "#     return f'rgba({int(cmap[0]*200)}, {int(cmap[1]*200)}, {int(cmap[2]*200)}, {alpha})'\n",
    "\n",
    "# # colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]  \n",
    "# colors = [matplotlib_to_plotly(plt.cm.Spectral(each), alpha=0.8) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "# fig = go.Figure()\n",
    "\n",
    "# for k, col in zip(unique_labels, colors):\n",
    "#     if k == -1:\n",
    "#         # col = 'rgba(0,0,0,0)'\n",
    "#         col = 'rgba(0,0,0,0)'\n",
    "\n",
    "#     class_member_mask = labels == k\n",
    "\n",
    "#     core_samples = full_info.iloc[:, 3:6][class_member_mask & core_samples_mask]\n",
    "#     fig.add_trace(go.Scatter3d(\n",
    "#         x=core_samples.iloc[:, 0],\n",
    "#         y=core_samples.iloc[:, 1],\n",
    "#         z=core_samples.iloc[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=6,\n",
    "#             color=col,\n",
    "#             opacity=0.8\n",
    "#         ),\n",
    "#         name=f'Cluster {k} Core'\n",
    "#     ))\n",
    "\n",
    "#     non_core_samples = full_info.iloc[:, 3:6][class_member_mask & ~core_samples_mask]\n",
    "#     fig.add_trace(go.Scatter3d(\n",
    "#         x=non_core_samples.iloc[:, 0],\n",
    "#         y=non_core_samples.iloc[:, 1],\n",
    "#         z=non_core_samples.iloc[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=6,\n",
    "#             color=col,\n",
    "#             opacity=0.5\n",
    "#         ),\n",
    "#         name=f'Cluster {k} Non-Core'\n",
    "#     ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=f\"Estimated number of clusters: {n_clusters_}\",\n",
    "#     margin=dict(l=0, r=0, b=0, t=0)\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99d17450-7cda-4a2d-98d4-9bc3393c11ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  0 -1  2]\n"
     ]
    }
   ],
   "source": [
    "from chi import *\n",
    "\n",
    "label_0 = full_info[full_info['label'] == 0]\n",
    "label_1 = full_info[full_info['label'] == 1]\n",
    "label_2 = full_info[full_info['label'] == 2]\n",
    "\n",
    "count_0 = get_count_dict(label_0)\n",
    "count_1 = get_count_dict(label_1)\n",
    "count_2 = get_count_dict(label_2)\n",
    "\n",
    "print(full_info['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbc93ba3-3001-4267-9855-fa6e04adacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01連接點數量 17\n",
      "02連接點數量 18\n",
      "12連接點數量 0\n",
      "各分群相加 6214\n",
      "各分群大小 (5628, 23) (540, 23) (46, 23)\n"
     ]
    }
   ],
   "source": [
    "full_0 = rbind_data.loc[count_0.keys()]\n",
    "full_1 = rbind_data.loc[count_1.keys()]\n",
    "full_2 = rbind_data.loc[count_2.keys()]\n",
    "\n",
    "lst01 = list(count_0.keys() & count_1.keys())\n",
    "lst02 = list(count_0.keys() & count_2.keys())\n",
    "lst12 = list(count_1.keys() & count_2.keys())\n",
    "# 將重複的key另外拉出進行分析，這裡drop是為了符合卡方的獨立性前提假設\n",
    "full_01 = full_0.loc[lst01]\n",
    "full_02 = full_0.loc[lst02]\n",
    "full_12 = full_1.loc[lst12]\n",
    "\n",
    "full_0 = full_0.drop(lst01, errors='ignore')\n",
    "full_0 = full_0.drop(lst02, errors='ignore')\n",
    "full_1 = full_1.drop(lst01, errors='ignore')\n",
    "full_1 = full_1.drop(lst12, errors='ignore')\n",
    "full_2 = full_2.drop(lst02, errors='ignore')\n",
    "full_2 = full_2.drop(lst12, errors='ignore')\n",
    "\n",
    "print('01連接點數量', len(lst01))\n",
    "for key1 in lst01:\n",
    "    del count_0[key1]\n",
    "    del count_1[key1]\n",
    "print('02連接點數量', len(lst02))\n",
    "for key2 in lst02:\n",
    "    del count_0[key2]\n",
    "    del count_2[key2]\n",
    "print('12連接點數量', len(lst12))\n",
    "for key3 in lst12:\n",
    "    del count_1[key3]\n",
    "    del count_2[key3]\n",
    "\n",
    "full_0 = add_count(full_0, count_0)\n",
    "full_1 = add_count(full_1, count_1)\n",
    "full_2 = add_count(full_2, count_2)\n",
    "\n",
    "print('各分群相加', full_0.shape[0] + full_1.shape[0] + full_2.shape[0])\n",
    "print('各分群大小', full_0.shape, full_1.shape, full_2.shape)\n",
    "# print('權重', full_0['count'].sum(), full_1['count'].sum(), full_2['count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcabbd40-4d5a-495a-b718-863ddecee51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_regression = [\n",
    "    '天候名稱',\n",
    "    '路面狀況-路面狀態名稱',\n",
    "    '肇因研判大類別名稱-主要', '當事者屬-性-別名稱', '當事者事故發生時年齡', \n",
    "    '車輛撞擊部位大類別名稱-最初',\n",
    "    '光線名稱',\n",
    "    '道路類別-第1當事者-名稱', \n",
    "    '速限-第1當事者', \n",
    "    '道路型態大類別名稱', \n",
    "    '事故位置大類別名稱',\n",
    "    '號誌-號誌種類名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱', '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '事故類型及型態大類別名稱',\n",
    "]\n",
    "\n",
    "def calculate_proportions(full, category_column):\n",
    "    # 計算受傷比例\n",
    "    grouped1 = full.groupby([category_column, '受傷']).size().unstack(fill_value=0)\n",
    "    total_count1 = grouped1.sum(axis=1)\n",
    "    proportions1 = grouped1.div(total_count1, axis=0) * 100\n",
    "    proportions1 = proportions1.round(2)  # 四捨五入到小數點後兩位\n",
    "    proportions1.columns = [f'受傷{i}' for i in range(grouped1.shape[1])]  # 更新列名稱\n",
    "\n",
    "    # 計算死亡比例\n",
    "    grouped2 = full.groupby([category_column, '死亡']).size().unstack(fill_value=0)\n",
    "    total_count2 = grouped2.sum(axis=1)\n",
    "    proportions2 = grouped2.div(total_count2, axis=0) * 100\n",
    "    proportions2 = proportions2.round(2)  # 四捨五入到小數點後兩位\n",
    "    proportions2.columns = [f'死亡{i}' for i in range(grouped2.shape[1])]  # 更新列名稱\n",
    "\n",
    "    # 合併兩個 DataFrame\n",
    "    final_df = proportions1.join(proportions2)\n",
    "    final_df['總數'] = total_count1\n",
    "    # 重置索引以將 category_column 作為一個普通列\n",
    "    final_df.reset_index(inplace=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4be8364a-5fe0-4ebe-be30-28e61e32d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangqiqian/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficients</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>wald_statistics</th>\n",
       "      <th>p_value</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>路面狀況-路面狀態名稱</th>\n",
       "      <td>1.983207</td>\n",
       "      <td>0.253039</td>\n",
       "      <td>7.837561</td>\n",
       "      <td>4.662937e-15</td>\n",
       "      <td>路面狀況-路面狀態名稱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>天候名稱</th>\n",
       "      <td>1.266694</td>\n",
       "      <td>0.411686</td>\n",
       "      <td>3.076845</td>\n",
       "      <td>2.092037e-03</td>\n",
       "      <td>天候名稱</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coefficients  standard_error  wald_statistics       p_value  \\\n",
       "路面狀況-路面狀態名稱      1.983207        0.253039         7.837561  4.662937e-15   \n",
       "天候名稱             1.266694        0.411686         3.076845  2.092037e-03   \n",
       "\n",
       "                 feature  \n",
       "路面狀況-路面狀態名稱  路面狀況-路面狀態名稱  \n",
       "天候名稱                天候名稱  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X01, y01, p01 = pval(full_0, full_1, lst_regression)\n",
    "p01[p01['p_value'] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cb599d2-f872-41d0-a2f6-bf1c11f1e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   features       VIF\n",
      "0                      天候名稱  2.852214\n",
      "1               路面狀況-路面狀態名稱  2.844998\n",
      "2              肇因研判大類別名稱-主要  1.078164\n",
      "3                當事者屬-性-別名稱  1.022823\n",
      "4                當事者事故發生時年齡  1.021078\n",
      "5            車輛撞擊部位大類別名稱-最初  1.060627\n",
      "6                      光線名稱  1.023345\n",
      "7             道路類別-第1當事者-名稱  1.038176\n",
      "8                  速限-第1當事者       NaN\n",
      "9                 道路型態大類別名稱  7.722232\n",
      "10                事故位置大類別名稱  7.641292\n",
      "11                號誌-號誌種類名稱  1.213776\n",
      "12         車道劃分設施-分向設施大類別名稱  1.079623\n",
      "13  車道劃分設施-分道設施-快車道或一般車道間名稱  1.208093\n",
      "14      車道劃分設施-分道設施-快慢車道間名稱  1.095880\n",
      "15       車道劃分設施-分道設施-路面邊線名稱  1.164547\n",
      "16             事故類型及型態大類別名稱  1.101906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangqiqian/opt/anaconda3/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1738: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in scalar divide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"features\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif\n",
    "\n",
    "# 假设 c0_for_lm_X 是你的预测变量DataFrame\n",
    "vif_df = calculate_vif(X01[lst_regression])\n",
    "print(vif_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
